# MLç¬”è®°(Lecture1&2)

> Lectured by HUNG-YI LEE (æå®æ¯…) Recorded by Yusheng zhaoï¼ˆyszhao0717@gmail.comï¼‰

***

\[TOC]

### æå®æ¯…ML2021é€Ÿè®°\_Lecture 1 & 2: Intro to ML/DL

#### Lecture 1: æœºå™¨å­¦ä¹ /æ·±åº¦å­¦ä¹ åŸºæœ¬æ¦‚å¿µç®€ä»‹

**æœºå™¨å­¦ä¹ åŸºæœ¬æ¦‚å¿µç®€ä»‹**

_Machine Learning $\approx$ Looking for Function_â€”â€”æœºå™¨å­¦ä¹ å°±æ˜¯è®©æœºå™¨ï¼ˆç¨‹åºï¼‰å…·å¤‡æ‰¾ä¸€ä¸ªå‡½æ•°çš„èƒ½åŠ›ã€‚

_Different types of Funtionsï¼š_

* Regressionï¼ˆå›å½’ï¼‰â€”â€”è¿ç»­ã€‚æœ€ç»ˆå¾—åˆ°æ ‡é‡ï¼ˆscalarï¼‰
* Classificationï¼ˆåˆ†ç±»ï¼‰â€”â€”ç¦»æ•£ã€‚å¾—åˆ°ä¸€ä¸ªé€‰æ‹©ï¼ˆoptions/classesï¼‰
* é™¤æ­¤ä¸¤å¤§ä»»åŠ¡å¤–ï¼Œè¿˜æœ‰**Structured Learning**ï¼šè®©æœºå™¨ä¸ä»…å­¦ä¼šåˆ†ç±»æˆ–è€…å®ç°é¢„æµ‹ä»»åŠ¡ï¼Œè€Œä¸”å¯ä»¥åˆ›é€ ç‰¹å®šçš„â€œæœ‰ç»“æ„â€çš„ç‰©ä½“ï¼Œè­¬å¦‚æ–‡ç« ã€å›¾åƒç­‰ã€‚

_æœºå™¨å­¦ä¹ å¦‚ä½•æ‰¾åˆ°è¿™ä¸ªå‡½æ•°ï¼Ÿï¼ˆä¸‰ä¸ªæ­¥éª¤ï¼‰_

*   **step 1ï¼šFunction with Unknown Parametersï¼š**

    è­¬å¦‚$y = b + wx\_1$ï¼Œè¯¥å‡è®¾æ–¹ç¨‹æ˜¯åŸºäº**domain knowledge**ï¼ˆé¢†åŸŸçŸ¥è¯†ï¼‰å„ç§å®šä¹‰ï¼š

    * **Model**ï¼šå¸¦æœ‰æœªçŸ¥çš„å‚æ•°(Parameters)çš„å‡½æ•°ï¼ˆfunctionï¼‰ã€‚
    * $x\_1$æ˜¯**feature**ï¼Œ$w$æ˜¯**weight**ï¼Œ$b$æ˜¯**bias**ï¼Œåä¸¤ä¸ªæœªçŸ¥å‚æ•°åŸºäºæ•°æ®(data)å­¦ä¹ å¾—åˆ°ã€‚
* **step 2ï¼šDefine Loss from Training Dataï¼š**
  * Lossï¼Œå³æŸå¤±å‡½æ•°ï¼Œä¸€ä¸ªä»…å¸¦æœ‰å‡½æ•°æœªçŸ¥çš„å‚æ•°çš„æ–¹ç¨‹ï¼Œè®°ä½œ$L(b,w)$
  * Lossçš„å€¼ä½“ç°äº†å‡½æ•°çš„ä¸€ç»„å‚æ•°çš„è®¾å®šçš„ä¼˜åŠ£
  *   é€šè¿‡è®­ç»ƒèµ„æ–™æ¥è®¡ç®—loss = |ä¼°æµ‹å€¼ - çœŸæ­£å€¼|ï¼Œ**Label**æŒ‡çš„å°±æ˜¯æ­£ç¡®çš„æ•°å€¼$\hat{y}$ï¼Œ$e\_i = |y - \hat{y}|,i = 1,2,..,n$ï¼Œæ‰€ä»¥ã€‚**Loss**ï¼š$L = \frac{1}{N}\sum\_n^{i=1}e\_i$ã€‚å…¶ä¸­ï¼Œå·®å€¼$e$çš„æœ‰ä¸åŒçš„è®¡ç®—æ–¹æ³•ï¼Œå¦‚ä¸Šé‡‡ç”¨ç›´æ¥åšå·®å¾—ç»å¯¹å€¼ï¼ˆMean Absolute Errorï¼šMAEï¼‰ï¼Œè¿˜æœ‰$e = (y-\hat{y})^2$ï¼Œå³Mean Square Errorï¼šMSEã€‚

      é€‰æ‹©å“ªä¸€ç§æ–¹æ³•è¡¡é‡$e$å–å†³äºæˆ‘ä»¬çš„éœ€æ±‚ä»¥åŠå¯¹äºtaskçš„ç†è§£ã€‚
  * æˆ‘ä»¬æšä¸¾ä¸åŒå‚æ•°ç»„åˆï¼ˆ$w,b$ï¼‰é€šè¿‡è®¡ç®—Losså€¼ç”»å‡ºç­‰é«˜çº¿å›¾ï¼š**Error Surface**
  * å¦‚æœ$y$å’Œ$\hat{y}$éƒ½æ˜¯æ¦‚ç‡==>**Cross-entropy**ï¼šäº¤å‰ç†µï¼Œé€šå¸¸ç”¨äºåˆ†ç±»ä»»åŠ¡
  * losså‡½æ•°è‡ªå®šä¹‰è®¾å®šï¼Œå¦‚æœæœ‰å¿…è¦çš„è¯ï¼Œlosså‡½æ•°å¯ä»¥outputè´Ÿå€¼
* **step 3ï¼šOptimization**
  * $w^_,b^_ = arg\space \underset{w,b}{min}L $
  * ä¸ºäº†å®ç°ä¸Šè¿°ä»»åŠ¡ï¼ˆæ‰¾åˆ°$w,b$ä½¿å¾—$L$æœ€å°ï¼‰,é€šå¸¸é‡‡ç”¨æ¢¯åº¦ä¸‹é™æ³•ï¼ˆ**Gradient Descent**ï¼‰ã€‚è­¬å¦‚ï¼šéšå»å…¶ä¸­ä¸€ä¸ªå‚æ•°![](https://s1.328888.xyz/2022/05/03/hgxn1.png)ä»è€Œå¾—åˆ°ä¸€ä¸ª$w-Loss(L)$çš„æ•°å€¼æ›²çº¿ï¼Œè®°ä½œ$L(w)$
    * **éšæœº**é€‰å–ä¸€ä¸ªåˆå§‹å€¼ï¼š$w\_0$
    *   è®¡ç®—ï¼š$\Large \frac{\part L}{\part w}|_{w=w\_0}$ï¼Œè¯¥ç‚¹ä½ç½®åœ¨Error Surfaceçš„åˆ‡çº¿æ–œç‡ï¼šè‹¥è´Ÿå€¼ï¼ˆNegativeï¼‰ï¼Œå·¦é«˜å³ä½=>$w$å³ç§»$\eta$ä½¿å¾—$Loss$å˜å°ï¼›è‹¥æ­£å€¼ï¼ˆPositiveï¼‰ï¼Œå·¦åº•å³é«˜=>$w$å·¦ç§»$\eta$ä½¿å¾—$Loss$å˜å°ã€‚æ–œç‡å¤§=>æ­¥ä¼$\eta$è·¨å¤§ä¸€äº›ï¼›æ–œç‡å°=>æ­¥ä¼$\eta$è·¨å°ä¸€äº›ã€‚$w\_1 \leftarrow w\_0 - \eta \large \frac{\part L}{\part w}|_{w=w\_0}$

        **$\eta$** : learning rateå­¦ä¹ ç‡ï¼Œå±äº**hyper parameters**ï¼šè¶…å‚æ•°ï¼Œè‡ªå·±è®¾å®šï¼Œå†³å®šæ›´æ–°é€Ÿç‡ã€‚
    *   ä¸æ–­è¿­ä»£æ›´æ¢$w$

        **â€œå‡â€é—®é¢˜**ï¼šå›¿äºå±€éƒ¨æœ€ä¼˜è§£local minimalï¼Œå¿½ç•¥äº†å®é™…çš„æœ€ä¼˜è§£global minimaï¼ˆä¸è¿‡å¹¶éæ¢¯åº¦ä¸‹é™æ³•çš„çœŸæ­£ç—›ç‚¹ï¼‰
  *   ç±»ä¼¼çš„ï¼Œå°†å•å‚æ•°éšæœºæ¢¯åº¦ä¸‹é™æ³•æ¨å¹¿åˆ°ä¸¤å‚æ•°ä¸Šï¼š$w^_,b^_ = arg\space \underset{w,b}{min}L $

      ![image-20210824153357084](https://s1.328888.xyz/2022/05/03/h9eDe.png)

      ç¡®å®š**æ›´æ–°æ–¹å‘ï¼š$(- \eta \large \frac{\part L}{\part w},- \eta \large \frac{\part L}{\part b})$**ï¼Œ$\eta$ä¸ºå­¦ä¹ ç‡

      æ€»ç»“æ¥è¯´ï¼ŒåŸºæœ¬æ­¥éª¤å¦‚ä¸‹

![image-20210824153852773](https://s1.328888.xyz/2022/05/03/h9uwO.png)

ä»¥ä¸Šä¸‰æ­¥æ˜¯æœºå™¨å­¦ä¹ æœ€ä¸ºåŸºæœ¬çš„æ¡†æ¶ã€‚åŸºäºæ­¤ï¼Œè¿˜éœ€è¦ç†è§£ä»»åŠ¡ï¼Œæ‘¸ç´¢æ•°æ®å˜åŒ–è§„å¾‹==>ä¿®æ”¹æ¨¡å‹ï¼ˆmodelï¼‰

***

**æ·±åº¦å­¦ä¹ åŸºæœ¬æ¦‚å¿µç®€ä»‹**

çº¿æ€§æ¨¡å‹ï¼ˆLinear Modelï¼‰è¿‡äºç®€å•ï¼Œæ— è®ºå‚æ•°ç»„åˆå¦‚ä½•å¯èƒ½æ€»æ˜¯æ— æ³•å®Œå…¨æ‹Ÿåˆä»»åŠ¡çš„Modelï¼Œè¿™é‡Œè¯´æ˜Linear Modelå…·æœ‰_severe limitation_ï¼Œè¿™ç§å±€é™è¢«ç§°ä¹‹ä¸º**Model Bias**ã€‚äºæ˜¯æˆ‘ä»¬éœ€è¦æ›´ä¸ºå¤æ‚çš„å‡½æ•°ã€‚

![](https://s1.328888.xyz/2022/05/03/h9yBq.png)

è¿™é‡Œç±»ä¼¼äºä½¿ç”¨**é˜¶è·ƒå‡½æ•°çš„ç»„åˆ**æ¥è¡¨ç¤ºåˆ†æ®µå‡½æ•°ï¼Œred curve= 1 + 2 + 3 + 0ï¼ˆå¸¸æ•°é¡¹ï¼‰,è¿™é‡Œå½’çº³å‡ºä¸€ä¸ªå¸¸è§çš„ç»“è®ºï¼šåˆ†æ®µå‡½æ•°$All\space Piecewise\space Linear\space Curves = constant$(å¸¸æ•°é¡¹) + ![](https://s1.328888.xyz/2022/05/03/h9g2P.png)

é‚£ä¹ˆï¼Œå¯¹äº$Beyond\space Piecewise\space Linear\space Curves$ï¼ˆè¿™ä¹Ÿæ˜¯æˆ‘ä»¬å¸¸è§çš„ä¸€èˆ¬å‡½æ•°çš„æ›²çº¿ï¼‰ï¼Œæˆ‘ä»¬ä½¿ç”¨è®¸å¤šå¤šä¸ä¸€æ ·çš„å°çº¿æ®µå»â€œé€¼è¿‘â€è¿ç»­çš„è¿™æ¡æ›²çº¿ï¼š

![](https://s1.328888.xyz/2022/05/03/h9JvA.png)

ä¸ºäº†è¡¨ç¤ºè¿™æ ·ä¸€ä¸ªè“è‰²çš„å‡½æ•°ï¼ˆå°çº¿æ®µï¼‰![](https://s1.328888.xyz/2022/05/03/h9XJS.png)ï¼ˆè¢«ç§°ä¹‹**Hard Sigmoid**ï¼‰ï¼Œè¿™é‡Œç”¨ä¸€ä¸ªå¸¸è§çš„æŒ‡æ•°å‡½æ•°æ¥é€¼è¿‘â€”â€”**Sigmoid Function**

$$
y = c \large \frac{1}{1 + e^{-(b+wx_1)}}= cÂ·sigmoid(b+wx_1)
$$

é€šè¿‡è°ƒæ•´$w,b,c$ï¼Œä¸€ç»„å‚æ•°ç»„åˆå¯ä»¥å¾—åˆ°ä¸åŒé€¼è¿‘çš„å°çº¿æ®µğŸ‘‡

![](https://s1.328888.xyz/2022/05/03/h9fzR.png)

è¿™ä¸ªå¼•å…¥è¶…çº§æ£’ï¼ï¼ç”±ä¸Šæ˜“çŸ¥ï¼Œä¸€ä¸ªè¿ç»­çš„å¤æ‚çš„å‡½æ•°æ›²çº¿å¯ä»¥è¢«åˆ†è§£æˆè®¸å¤šç¦»æ•£çš„å°çº¿æ®µï¼ˆ**Hard Sigmoid**ï¼‰å’Œä¸€ä¸ªå¸¸æ•°é¡¹çš„çº¿æ€§ç›¸åŠ ï¼Œç„¶åæ¯ä¸ªå°çº¿æ®µè¢«ä¸€ä¸ªä¸‰å‚æ•°çš„**Sigmoid Function**æ‰€é€¼è¿‘ã€‚ä¸‹å›¾çš„å‡½æ•°æ›²çº¿å¯ä»¥è¡¨ç¤ºä¸ºä¸€ä¸ªå«æœ‰10ä¸ªæœªçŸ¥å‚æ•°çš„modeï¼š

![](https://s1.328888.xyz/2022/05/03/h9i1i.png)

ä»è€Œï¼Œå¯ä»¥äº§ç”Ÿä¸€ä¸ªä»ç®€å•->å¤æ‚ã€å•ä¸€->å¤šå…ƒçš„å‡½æ•°æ¨¡å¼ã€‚æ–°çš„æ¨¡å‹åŒ…å«æ›´å¤šçš„ç‰¹å¾ã€‚

$$
y=b+wx_1 \Rightarrow y = b + \underset{i}{\sum}c_i sigmoid(b_i+w_ix_1)
$$

ç”±ï¼ˆ2ï¼‰å¼ï¼Œè€ƒè™‘åˆ°å¤šç‰¹å¾å› ç´ ï¼Œè¿›ä¸€æ­¥æ‰©å±•å¾—

$$
y = b + \underset{j}{\sum}w_jx_j \Rightarrow y = b + \underset{i}{\sum}c_i sigmoid(b_i+\underset{j}{\sum}w_{ij}x_1)
$$

å…¶ä¸­$i$è¡¨ç¤º$i^{th}$ä¸ª$Sigmoid$å‡½æ•°ï¼ˆæ¨¡å‹çš„åŸºå‡½æ•°ä¸ªæ•°ï¼‰ï¼Œ$x\_j$è¡¨ç¤ºä¸€ä¸ªå‡½æ•°ä¸­ä¸åŒçš„ç‰¹å¾æˆ–è€…é¢„æµ‹çš„æ•°æ®é•¿åº¦ï¼Œï¼Œ$w\_j$è¡¨ç¤ºå¯¹åº”ç‰¹å¾æƒå€¼ã€‚

![](https://s1.328888.xyz/2022/05/03/h92ev.png)

æ€»ç»“ï¼šåœ¨é€šç”¨çš„æœºå™¨å­¦ä¹ æ•™ç¨‹ä¸­ï¼Œ$sigmoid$å‡½æ•°æ™®éè¢«è§†ä½œä¸€æ¬¾å¸¸è§çš„æ¿€æ´»å‡½æ•°ï¼Œåœ¨æœ¬è¯¾ç¨‹ä¸­ï¼Œä»ä»£è¡¨ä»»åŠ¡æ¨¡å‹çš„éçº¿æ€§å‡½æ•°å‡ºå‘-->æé™ï¼šåˆ†æ®µçš„çº¿æ€§å‡½æ•°ç»„åˆ-->ä¸åŒæ€§è´¨/ç‰¹å¾çš„$sigmoid$å‡½æ•°é€¼è¿‘å°åˆ†å‰²çš„çº¿æ€§å‡½æ•°ã€‚å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œæˆ‘ä»¬æœ‰ä¸‰ä¸ªæ¿€æ´»å‡½æ•°ï¼ˆ$sigmoid \space function$ï¼‰ä»¥åŠè¾“å‡ºçš„ä¸€ä¸ªæ–¹ç¨‹ç»„ï¼ˆçŸ©é˜µ/å‘é‡ç›¸ä¹˜è¡¨ç¤ºï¼‰ï¼Œè¿™é‡ŒåŸºæœ¬ä¸Šå¯ä»¥è§†ä¸ºä¸€ä¸ªå…·æœ‰ä¸‰ä¸ªç¥ç»å…ƒçš„å…¨è¿æ¥çš„ä¸€å±‚ç¥ç»ç½‘ç»œã€‚

$$
[r_1,r_2,r_3]^T = [b_1,b_2,b_3]^T + \begin{bmatrix}w_{11},w_{12},w_{13}\\w_{21},w_{22},w_{23}\\w_{31},w_{32},w_{33} \end{bmatrix}Â·[x_1,x_2,x_3]^T
$$

æ€»ä¹‹ï¼Œ

$$
r = \mathbb {b} +wÂ·x
$$

æ¥ä¸‹æ¥ï¼Œå°†è¯¥æ–¹ç¨‹ç»„$r$é€šè¿‡æ¿€æ´»å‡½æ•°è¾“å‡ºå‘é‡$a$ï¼Œè¿™é‡Œ

$$
a = \sigma(r)
$$

![](https://s1.328888.xyz/2022/05/03/h9A3J.png)

ç”±(5)ã€(6)å¾—

$$
\spaceç”± a= \sigma(\mathbb{b} + wÂ·x) \\\Rightarrow y = b + [c_1,c_2,c_3]Â·\sigma(\mathbb{b} + wÂ·x)
$$

æ³¨æ„ï¼Œ$\sigma$ä¸­çš„$\mathbb{b}$æ˜¯å‘é‡ï¼Œå¤–é¢çš„$b$æ˜¯æ•°å€¼ï¼Œç»“æœ$y$ä¹Ÿæ˜¯æ•°å€¼ï¼ˆæ ‡é‡ï¼‰ã€‚

**==Step 1ï¼šunknown parametersçš„å¼•å…¥==**

åœ¨ä¸Šè¿°ä¾‹å­ä¸­ï¼Œ$\mathbb{x}$è¡¨ç¤ºç‰¹å¾ï¼Œ$\mathbb{c}ã€\mathbb{b}ã€Wã€b$ä¸ºæœªçŸ¥å‚æ•°ã€‚ä¸ºäº†æŠŠæœªçŸ¥å‚æ•°ç»Ÿä¸€èµ·æ¥å¤„ç†ï¼Œæˆ‘ä»¬è¿›è¡Œå¦‚ä¸‹æ³›åŒ–ï¼Œæ¯”æ–¹è¯´ï¼Œ$\theta\_1 = \[c\_1,b\_1,w\_{11},w\_{12},w\_{13},b]^T$

![](https://s1.328888.xyz/2022/05/03/h9hBF.png)

$\mathbb{\theta}$æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„å‘é‡ï¼Œé‡Œé¢çš„ç¬¬ä¸€ä¸ªå‘é‡ä¸º$\theta\_1$ï¼Œä»¥æ­¤ç±»æ¨ã€‚åªè¦æ˜¯æœªçŸ¥å‚æ•°éƒ½ç»Ÿç§°åœ¨$\theta$å†…ã€‚

åœ¨å‚æ•°å¾ˆå°‘çš„æ—¶å€™ï¼Œå¯ä»¥ç›´æ¥ç©·ä¸¾å‚æ•°ç»„åˆï¼Œå¯»æ‰¾æœ€ä¼˜è§£ï¼›ä½†æ˜¯å½“æœºå™¨å­¦ä¹ é—®é¢˜ä¸­çš„å‚æ•°è¾ƒå¤šæ—¶ï¼Œæ¢¯åº¦ä¸‹é™æ³•æ›´ä¸ºåˆç†ã€‚éšå«å±‚ç¥ç»å…ƒèŠ‚ç‚¹ä¸ªæ•°ï¼ˆ$sigmoid$å‡½æ•°ä¸ªæ•°ï¼‰è‡ªå·±å†³å®šï¼Œå…¶æœ¬èº«ä¸ªæ•°æ•°å€¼ä¹Ÿä¸ºè¶…å‚æ•°ä¹‹ä¸€ã€‚

**==Step 2ï¼šç¡®å®šlosså‡½æ•°==**

* lossæ˜¯ä¸€ä¸ªæœªçŸ¥å‚æ•°çš„å‡½æ•°ï¼š$L(\mathbb{\theta})$
* lossè¡¡é‡ä¸€ç»„å‚æ•°å€¼è¡¨ç¤ºæ¨¡å‹æ•ˆæœä¼˜åŠ£

![](https://s1.328888.xyz/2022/05/03/h9H8W.png)

åŒä»¥ä¸Šä»‹ç»çš„æ­¥éª¤æ— åŒºåˆ«ã€‚

**==Step 3ï¼šOptimization==**

æ–°æ¨¡å‹çš„çš„optimizationæ­¥éª¤å’Œä¹‹å‰ä»‹ç»çš„æ— ä»»ä½•åŒºåˆ«ã€‚å¯¹äº$\mathbb{\theta}=\[\theta\_1,\theta\_2,\theta\_3...]^T$

* éšæœºé€‰å–åˆå§‹å€¼$\mathbb{\theta}^0$ï¼Œ**gradient**æ¢¯åº¦è®°ä¸º$\large\mathbb{\mathcal{g\}}=\[\frac{\partial L}{\partial \theta\_1}_{|\mathbb{\theta}=\mathbb{\theta}^0},\frac{\partial L}{\partial \theta\_2}_{|\mathbb{\theta}=\mathbb{\theta}^0},...]^T$ï¼Œå¯ç®€åŒ–ä¸º$\mathbb{\mathcal{g\}}=\nabla L(\mathbb{\theta}^0)$å‘é‡é•¿åº¦=å‚æ•°ä¸ªæ•°ã€‚
*   æ›´æ–°å‚æ•°ğŸ‘‡($\eta$å½“ç„¶æ˜¯å­¦ä¹ ç‡å•¦)

    $$
    \mathbb{\theta}=[\theta_1^1,\theta_2^1,...]^T \leftarrow \mathbb{\theta}=[\theta_1^0,\theta_2^0,...]^T - [\textcolor{red}\eta\frac{\partial L}{\partial \theta_1}_{|\mathbb{\theta}=\mathbb{\theta}^0},\textcolor{red}\eta\frac{\partial L}{\partial \theta_2}_{|\mathbb{\theta}=\mathbb{\theta}^0},...]^T \\ \mathbb{\theta}^1 \leftarrow \mathbb{\theta}^0 - \textcolor{red}\eta \mathbb{\mathcal{g}}
    $$

    ä¸æ–­è¿­ä»£$\mathbb{\theta}^2 \leftarrow \mathbb{\theta}^1 - \textcolor{red}\eta \mathbb{\mathcal{g\}},\mathbb{\theta}^3 \leftarrow \mathbb{\theta}^2 - \textcolor{red}\eta \mathbb{\mathcal{g\}},...$ï¼Œç›´åˆ°æ‰¾åˆ°ä¸æƒ³åšæˆ–è€…æ¢¯åº¦æœ€åæ˜¯zero vectorï¼ˆåè€…ä¸å¤ªå¯èƒ½ï¼‰ã€‚

å®é™…ä¸Šåœ¨åšæ¢¯åº¦ä¸‹é™çš„æ—¶å€™ï¼Œæˆ‘ä»¬è¦æŠŠæ•°æ®$N$åˆ†æˆè‹¥å¹²**Batch**ï¼ˆç§°ä¹‹ä¸º**æ‰¹é‡**ï¼‰,å¦‚ä½•åˆ†ï¼Ÿéšä¾¿åˆ†ã€‚åŸå…ˆæ˜¯æŠŠæ‰€æœ‰dataæ‹¿æ¥ç®—ä¸€ä¸ªlossï¼Œç°åœ¨æ˜¯åœ¨ä¸€ä¸ªBatchä¸Šç®—lossï¼Œé‚£ä¹ˆå¯¹äº$B\_1,B\_2,...$æˆ‘ä»¬å¯ä»¥å¾—åˆ°$L^1,L^2,...$

![](https://s1.328888.xyz/2022/05/03/h9qRy.png)

æŠŠæ‰€æœ‰batchç®—è¿‡ä¸€æ¬¡ï¼Œç§°ä¹‹ä¸ºä¸€ä¸ª**epoch**ï¼š1 **epoch** = see all the batches onceã€‚ä»¥ä¸Šå³ä¸º**æ‰¹é‡æ¢¯åº¦ä¸‹é™**ã€‚æ³¨æ„åŒºåˆ«ï¼šä¸€æ¬¡updateæŒ‡çš„æ˜¯æ¯æ¬¡æ›´æ–°ä¸€æ¬¡å‚æ•°ï¼Œè€ŒæŠŠæ‰€æœ‰çš„Batchçœ‹è¿‡ä¸€éåˆ™æ˜¯epochã€‚

å¦å¤–ï¼Œ**Batch Size**å¤§å°ä¹Ÿæ˜¯ä¸€ä¸ªè¶…å‚æ•°ã€‚

**å¯¹æ¨¡å‹åšæ›´å¤šçš„å˜å½¢ï¼š**

$Sigmoid \rightarrow ReLU$ï¼š**Rectified Linear Unitï¼ˆReLUï¼‰**ï¼š$cÂ·max(0,b+wx\_1)$æ›²çº¿ã€‚ä¸åŒçš„æ˜¯ï¼Œæˆ‘ä»¬éœ€è¦ä¸¤ä¸ª$ReLU$æ›²çº¿æ‰èƒ½åˆæˆä¸€ä¸ª**Hard Sigmoid**å‡½æ•°æ›²çº¿ï¼ˆè“è‰²çš„å°çº¿æ®µï¼‰ã€‚æ— è®ºæ˜¯$Sigmoid$è¿˜æ˜¯$ReLU$éƒ½æ˜¯**æ¿€æ´»å‡½æ•°ï¼ˆActivation Functionï¼‰**ã€‚

ä¸Šé¢çš„é•¿ç¯‡å¤§è®ºä»…ä»…è®²è¿°äº†ä¸€å±‚ç¥ç»ç½‘ç»œæ˜¯å¦‚ä½•æ­å»ºçš„ï¼Œé‚£ä¹ˆå¤šå±‚ç¥ç»ç½‘ç»œçš„è€¦åˆï¼ˆæˆ–è€…æ˜¯é€æ­¥æ„å»ºéšè—å±‚ï¼‰$\rightarrow$**æ·±åº¦å­¦ä¹ ï¼ˆDeep Learning**ï¼‰ã€‚è¿™é‡Œçš„å±‚æ•°ä¹Ÿæ˜¯ä¸ªè¶…å‚æ•°ï¼Œå±‚æ•°è¶Šå¤šï¼Œå‚æ•°è¶Šå¤šã€‚

åŒä¸€å±‚å¥½å¤šä¸ªæ¿€æ´»å‡½æ•°ï¼ˆNeruonï¼‰å°±æ˜¯ä¸€ä¸ªhidden layerï¼Œå¤šä¸ªhidden layerç»„æˆäº†Neural Networkã€‚è¿™ä¸€æ•´å¥—æŠ€æœ¯å°±æ˜¯deep learningã€‚

ä¹‹åçš„ç¥ç»ç½‘ç»œå±‚æ•°è¶Šæ¥è¶Šå¤šï¼ˆAlexNetã€GoogLeNetç­‰ç­‰ï¼‰é‚£ä¹ˆä¸ºä½•æ˜¯**æ·±**åº¦å­¦ä¹ è€Œä¸æ˜¯\*\*å®½ï¼ˆè‚¥ï¼‰**åº¦å­¦ä¹ çš„ç¼˜ç”±ã€‚å¦å¤–ï¼Œéšç€å±‚æ•°å˜å¤šï¼Œå‘ç”Ÿ**overfittingï¼ˆè¿‡æ‹Ÿåˆï¼‰\*\*çš„ç°è±¡ã€‚è¿™äº›æ˜¯æˆ‘ä»¬ä¹‹åè¯¾ç¨‹è¦è®¨è®ºçš„é—®é¢˜ã€‚

***

#### Lecture 2ï¼šæœºå™¨å­¦ä¹ ä»»åŠ¡æ”»ç•¥â€”â€”å¦‚ä½•è®­ç»ƒå¥½æˆ‘ä»¬çš„ç¥ç»ç½‘ç»œ:-ï¼‰

Training Data$\Large \Rightarrow$Trainingï¼ˆLecture 1ï¼šä¸‰ä¸ªæ­¥éª¤ï¼‰$\Large \Rightarrow$**Testing data**

![](https://s1.328888.xyz/2022/05/03/h9513.png)

_**1.ä» loss on training data ç€æ‰‹**_

**==1.1Model Bias==**

æ¨¡å‹è¿‡äºç®€å•æˆ–è€…ä¸å®é™…ç›¸å·®è¿‡å¤šï¼Œæ— è®ºå¦‚ä½•è¿­ä»£ï¼Œlosså€¼æ— æ³•é™ä½ã€‚éœ€è¦è®©æ¨¡å‹æ›´åŠ flexibleã€‚ä¸€å®šèŒƒå›´å†…ï¼Œå±‚æ•°è¶Šå¤šæ¨¡å‹è¶Šæœ‰å¼¹æ€§ã€‚

![](https://s1.328888.xyz/2022/05/03/h9Dvk.png)

**==1.2ä¼˜åŒ–é—®é¢˜ï¼ˆOptimization Issueï¼‰==**

**å¯»æ‰¾lossé™·å…¥å±€éƒ¨æœ€ä¼˜è§£**

![](https://s1.328888.xyz/2022/05/03/h9lLd.png)

å…³äºä¸¤è€…çš„æ¯”è¾ƒå’Œåˆ¤æ–­ï¼Œä»‹ç»äº†æ–‡ç« \[Population imbalance in the extended Fermi-Hubbard model]\(\[[1512.00338\] Population imbalance in the extended Fermi-Hubbard model (arxiv.org)](https://arxiv.org/abs/1512.00338))å½“ä¸¤ä¸ªç½‘ç»œAã€Bï¼ŒAåœ¨Bçš„åŸºç¡€ä¸Šæœ‰æ›´å¤šçš„å±‚æ•°ï¼Œä½†æ˜¯åœ¨ä»»åŠ¡ä¸ŠAçš„lossè¦æ¯”Bå¤§ï¼Œè¿™è¯´æ˜Aç½‘ç»œçš„Optimizationæ²¡æœ‰åšå¥½ã€‚

ä»å¯¹æ¯”ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥è·å¾—æ›´ç¡®åˆ‡çš„è®¤çŸ¥ï¼›æˆ‘ä»¬å¯ä»¥ä»è¾ƒä¸ºæµ…çš„modelå¼€å§‹ç€æ‰‹ï¼›å¦‚æœæ›´æ·±çš„ç½‘ç»œå¹¶æ²¡æœ‰å¾—åˆ°æ›´å°çš„lossï¼Œé‚£ä¹ˆè¯¥ç½‘ç»œæœ‰optimization issue

å½“æˆ‘ä»¬åœ¨training dataä¸Šå¾—åˆ°è‰¯å¥½çš„lossï¼Œæˆ‘ä»¬å°±å¯ä»¥ç€æ‰‹åœ¨testing dataä¸Šé™ä½loss

_**2.ä» loss on testinging data ç€æ‰‹**_

**==2.1 overfitting è¿‡æ‹Ÿåˆ==**

* å¢åŠ training dataï¼ˆä½œä¸šé‡Œä¸è¡Œï¼‰
* Data Augmentationï¼Œæ ¹æ®è‡ªå·±å¯¹ä»»åŠ¡çš„ç†è§£ï¼Œäººä¸ºåˆ›é€ å‡ºä¸€äº›æ–°çš„æ•°æ®ã€‚ä¾‹å¦‚ï¼šå›¾åƒè¯†åˆ«è®­ç»ƒä¸­å¯ä»¥æŠŠè®­ç»ƒå›¾ç‰‡å·¦å³ç¿»è½¬ï¼Œè£å‰ªè·å¾—æ–°çš„è®­ç»ƒæ•°æ®
* ç»™äºˆæ¨¡å‹ä¸€å®šé™åˆ¶ï¼Œä½¿å…¶ä¸é‚£ä¹ˆflexible
  * æ›´å°‘çš„å‚æ•°
  * æ›´å°‘çš„features
  * Early stoppingã€Regularizationã€Dropoutï¼ˆLecture 4ï¼‰

Bias-Complexity Trade-offï¼šæ¨¡å‹å¤æ‚çš„ç¨‹åº¦ï¼ˆæˆ–æ›°æ¨¡å‹çš„å¼¹æ€§ï¼‰â€”â€”functionæ¯”è¾ƒå¤šï¼Œéšç€å¤æ‚åº¦å¢åŠ ï¼Œtrainingçš„lossè¶Šæ¥è¶Šå°ï¼Œç„¶è€Œtestingçš„lossæ˜¯ä¸€ä¸ªå‡¹çŠ¶çš„æ›²çº¿ï¼ˆå…ˆå°åå¤§ï¼‰ã€‚

![](https://s1.328888.xyz/2022/05/03/h9zu4.png)

> æœºå™¨å­¦ä¹ æ¯”èµ›ï¼ˆä¾‹å¦‚Kaggleï¼‰åˆ†ä¸ºä¸¤ä¸ªLeaderboardï¼špublicå’Œprivateï¼ˆAã€Bæ¦œï¼‰ï¼Œåœ¨ä¸¤ä¸ªæµ‹è¯•é›†ä¸Šçš„åˆ†æ•°çš„å·®åˆ«è¿‡å¤§åœ¨äºmodelä¸å¤Ÿé²æ£’ã€‚æ¢è¨€ä¹‹ï¼Œåœ¨å…¬ç”¨æ•°æ®é›†ä¸Šè¾¾åˆ°è¾ƒé«˜çš„å‡†ç¡®ç‡ï¼Œä¸è§å¾—åœ¨è½åœ°ä½¿ç”¨ä¸Šèƒ½å®Œå…¨å®ç°å…¶æµ‹è¯•çš„levelï¼ˆéª—éª—éº»ç“œçš„å•†ä¸šèœœå£ï¼‰ã€‚
>
> æ¯æ—¥é™åˆ¶ä¸Šä¼ æ¬¡æ•°ä¸»è¦æ˜¯ä¸ºäº†é˜²æ­¢å„ä½æ°´æ¨¡å‹ä¸æ–­testå…¬ç”¨æ•°æ®é›†åˆ·åˆ†æ•°ï¼ˆæ— æ„ä¹‰\~\~ï¼‰

**Cross Validation äº¤å‰éªŒè¯**

æŠŠtraining dataåˆ†æˆä¸¤åŠï¼štraining dataå’Œvalidation dataã€‚ å¦‚ä½•åˆ†å‘¢ï¼Ÿå¯ä»¥éšæœºåˆ†ï¼›å¦å¤–ï¼Œå¯ä»¥ç”¨**N-æŠ˜äº¤å‰éªŒè¯ï¼ˆN-fold Cross Validationï¼‰**

![](https://s1.328888.xyz/2022/05/03/h9CDB.png)

**==2.2 mismatch==**

Mismatchè¡¨ç¤ºè®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®çš„\*\*åˆ†å¸ƒï¼ˆdistributionsï¼‰\*\*ä¸ä¸€è‡´ã€‚

ä¹Ÿå¯ä»¥è®¤ä¸ºæ˜¯ä¸€ç§overfittingã€‚é€šå¸¸åœ¨é¢„å®šçš„æœºå™¨å­¦ä¹ ä»»åŠ¡ä¸­ä¸ä¼šå‡ºç°ã€‚

ï¼ˆHW11é’ˆå¯¹è¿™ä¸ªé—®é¢˜ï¼‰

***

#### Lecture 2\*ï¼šå¦‚ä½•è®­ç»ƒå¥½ç±»ç¥ç»ç½‘ç»œ

**When gradient is small: Local Minimum and Saddle Point**

å¦‚æœOptimizationå¤±è´¥äº†...â€”â€”éšç€ä¸æ–­updateè€Œtraining lossä¸å†ä¸‹é™ï¼Œä½ ä¸æ»¡æ„å…¶è¾ƒå°å€¼ï¼›æˆ–è€…ä¸€å¼€å§‹updateæ—¶lossä¸‹é™ä¸ä¸‹å»

Whyï¼Ÿâ€”â€”å¾ˆæœ‰å¯èƒ½updateåˆ°ä¸€ä¸ªåœ°æ–¹ï¼ˆ**critical point**ï¼‰ï¼Œgradientå¾®åˆ†åå‚æ•°ä¸º0ï¼ˆæˆ–ç›¸å½“æ¥è¿‘0ï¼‰

![](https://s1.328888.xyz/2022/05/03/h9p3T.png)

è¿™ä¸ªç‚¹å¯èƒ½æ˜¯**local minima**æˆ–æ˜¯**saddle pointï¼ˆéç‚¹ï¼‰**

é‚£ä¹ˆï¼Œå¦‚ä½•çŸ¥é“è¿™ä¸ªç‚¹ï¼ˆ**critical point**ï¼‰æ˜¯ä¸Šè¿°ä¸¤ç§çš„å“ªä¸€ç§ï¼Ÿï¼ˆæ•°å­¦ä¸Šåˆ†æå¦‚ä¸‹ï¼‰

> #### Tayler Series Approximation
>
> å¯¹äº$L(\theta)$ ï¼Œå½“ $\theta \approx \theta'$ æ—¶ï¼Œä»¥ä¸‹å¯ä»¥çº¦ä¸ºæˆç«‹ï¼š
>
> $L(\theta) \approx L(\theta')+(\theta-\theta')^Tg+\frac{1}{2}(\theta-\theta')^TH(\theta-\theta')$
>
> ![](https://s1.328888.xyz/2022/05/03/h90E2.png)
>
> * æ¢¯åº¦**Gradient**$g$æ˜¯å‘é‡ï¼Œç”¨æ¥å¼¥è¡¥ $\theta$ å’Œ$\theta'$ä¹‹é—´çš„å·®è·ã€‚ $g =\nabla L(\theta') ,g\_i = \Large \frac{\partial L(\theta')}{\partial \theta\_i}$
> * **Hessian**$H$æ˜¯ä¸€ä¸ªçŸ©é˜µã€‚$H\_{ij}=\Large \frac{\partial ^2}{\partial \theta\_i \partial \theta\_j}\small L(\theta')$ï¼Œå³$L$çš„äºŒæ¬¡å¾®åˆ†ï¼ˆæµ·å¡çŸ©é˜µï¼‰
>
> #### Hessian
>
> ![](https://s1.328888.xyz/2022/05/03/h978M.png)
>
> å½“æ¢¯åº¦$g$ä¸º0æ—¶ï¼Œä»¤$(\theta-\theta')=v$ï¼šâ‘ å¯¹äºä»»ä½•å¯èƒ½çš„$v$ï¼Œè‹¥éƒ½æœ‰$v^THv>0$ï¼Œæ‰€ä»¥$L(\theta)>L(\theta')$ï¼Œè¯´æ˜æ˜¯**Local minima**ï¼Œç­‰ä»·äº$H$æ˜¯ä¸€ä¸ªç§°ä¹‹ä¸º_positive definite_çš„çŸ©é˜µï¼ˆå…¶æ‰€æœ‰ç‰¹å¾å€¼\[_eigenvalue_]ä¸ºæ­£ï¼‰ï¼Œç”±æ­¤ä¹Ÿå¯ä»¥åˆ¤æ–­æ˜¯å¦**local minima**ï¼›â‘¡å¯¹äºä»»ä½•å¯èƒ½çš„$v$ï¼Œè‹¥éƒ½æœ‰$v^THv<0$ï¼Œæ‰€ä»¥$L(\theta)\<L(\theta')$ï¼Œè¯´æ˜æ˜¯**Local maxima**ï¼Œç­‰ä»·äº$H$æ˜¯ä¸€ä¸ªç§°ä¹‹ä¸º_negative definite_çš„çŸ©é˜µï¼ˆå…¶æ‰€æœ‰ç‰¹å¾å€¼\[_eigenvalue_]ä¸ºè´Ÿï¼‰ï¼Œç”±æ­¤ä¹Ÿå¯ä»¥åˆ¤æ–­æ˜¯å¦**local maxima**ï¼›â‘¢å¯¹äºä»»ä½•å¯èƒ½çš„$v$ï¼Œå¯èƒ½$v^THv>0$ï¼Œä¹Ÿå¯èƒ½$v^THv<0$ï¼Œè¯´æ˜æ˜¯**saddle point**ã€‚ç­‰ä»·äºçŸ©é˜µæœ‰ç€$H$çš„ç‰¹å¾å€¼æœ‰æ­£æœ‰è´Ÿã€‚

æ‰€ä»¥ï¼Œå¦‚æœæ›´æ–°æ—¶èµ°åˆ°äº†saddle pointï¼Œè¿™æ—¶å€™æ¢¯åº¦ä¸º0ï¼Œé‚£ä¹ˆå°±å¯ä»¥çœ‹$H$ï¼šï¼ˆ$H$å¯ä»¥å‘Šè¯‰æˆ‘ä»¬å‚æ•°æ›´æ–°çš„æ–¹å‘ï¼‰

> $\mathbb{u}$æ˜¯$H$çš„ç‰¹å¾å‘é‡ï¼Œ$\lambda$æ˜¯$\mathbb{u}$çš„ç‰¹å¾å€¼ã€‚$\Large \Rightarrow$ $\mathbb{u}^TH\mathbb{u} = \mathbb{u}^T(\lambda\mathbb{u}) =\lambda||\mathbb{u}||^2$ $(\*) $
>
> è‹¥$\lambda<0$ï¼Œé‚£ä¹ˆ$(\*)<0$ï¼Œ$\Large \Rightarrow$ $L(\theta)\<L(\theta')$ï¼Œè¿™é‡Œå‡è®¾$\theta-\theta'=\mathbb{u}$ï¼Œå³åªè¦è®©ä¸‹ä¸€æ­¥æ›´æ–°åˆ°$\theta = \theta'+\mathbb{u}$ï¼Œ$L$å°±ä¼šå˜å°ã€‚
>
> å¦‚ä¸Šï¼Œéœ€è¦è®¡ç®—äºŒæ¬¡å¾®åˆ†ï¼Œè®¡ç®—é‡è¾ƒå¤§ï¼Œæ‰€ä»¥ä¹‹åä¼šæœ‰è®¡ç®—é‡æ›´å°çš„æ–¹æ³•ã€‚

ä¹‹åï¼Œè€å¸ˆè®²äº†ä¸‰ä½“é‡Œçš„ä¸€ä¸ªæ•…äº‹ï¼ˆé­”æœ¯å¸ˆï¼Œå›å£«å¦ä¸å ¡ï¼‰ï¼Œæ·¦ã€‚ã€‚ã€‚å¼•å…¥äº†åœ¨é«˜ç»´ç©ºé—´æä¾›å‚æ•°å­¦ä¹ çš„è§†è§’ã€‚å‚æ•°è¶Šå¤šï¼Œerror surfaceç»´åº¦è¶Šæ¥è¶Šé«˜ã€‚å½“åœ¨ä¸€ä¸ªç›¸å½“çš„ç»´åº¦ä¸‹åšè®­ç»ƒä»»åŠ¡æ—¶ï¼Œå¦‚æœupdateä¸‹å»lossä¸å†ä¸‹é™ï¼Œå¤§æ¦‚ç‡æ˜¯å¡åœ¨äº†saddle pointä¸Šï¼Œlocal minimaå¹¶æ²¡æœ‰å¦‚æ­¤å¸¸è§ã€‚

**Tips For trainingï¼šBATCH and MOMENTUM**

**==å…³äºBATCH==**

å›é¡¾ä¹‹å‰çš„ä»‹ç»ï¼ˆLecture 1ï¼‰,1 **epoch** = see all the batches once $\rightarrow$ **Shuffle** after each epochï¼Œå³åœ¨æ¯ä¸€æ¬¡epochå¼€å§‹ä¹‹å‰éƒ½ä¼šåˆ†ä¸€æ¬¡batchï¼Œå¯¼è‡´æ¯æ¬¡epochçš„batcheséƒ½ä¸å®Œå…¨ä¸€æ ·ã€‚Batchå¤§å°çš„è®¾ç½®å¯ä»¥åˆ†æˆä¸¤ç§æƒ…å†µã€‚

_==Small Batch v.s. Large Batch==_ï¼Œå‡è®¾æ€»æ•°ä¸ºN=20ï¼š

![](https://s1.328888.xyz/2022/05/03/h9IR7.png)

ä¸¤è€…éƒ½å¾ˆæç«¯ï¼Œå·¦è¾¹å°±çœ‹ä¸€éï¼Œè“„åŠ›å¤ªé•¿ï¼›è€Œå³è¾¹ï¼Œçœ‹ä¸€ä¸ªå°±è“„åŠ›ä¸€æ¬¡ï¼Œé¢‘ç¹ç¬å‘ï¼Œæ–¹å‘ä¸å®šï¼Œä¹±æªæ‰“é¸Ÿã€‚

ç®—åŠ›çš„è¿›æ­¥å¸¦æ¥å¹¶è¡Œè®¡ç®—çš„èƒ½åŠ›å¢å¼ºâ‘ åœ¨å¦‚ä¸Šæ¡ä»¶ä¸‹ï¼Œepochè¾ƒå¤§çš„batchçš„è®­ç»ƒé€Ÿåº¦å¯ä»¥æ›´å¿«ï¼ˆåç›´è§‰ï¼‰ã€‚â‘¡è€Œå°ä¸€ç‚¹çš„batchçš„Optimizationçš„ç»“æœä¼šæ›´å¥½ã€‚ï¼ˆå¯èƒ½çš„è§£é‡Šï¼šloss functionæ˜¯ç•¥æœ‰å·®å¼‚çš„ï¼Œå³ä½¿updateåˆ°äº†critical pointï¼Œä¸å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜è§£ï¼‰ï¼›â‘¢åœ¨ä¸¤è€…batchä¸Štrainçš„æ•ˆæœç›¸è¿‘ï¼Œè€Œtestç»“æœç›¸å·®å¾ˆå¤§ï¼ˆå¤§batchè¾ƒå·®ï¼‰ï¼Œè¯´æ˜å‘ç”Ÿoverfittingã€‚å°çš„batchçš„æ³›åŒ–æ€§æ›´å¥½äº›ã€‚

![](https://s1.328888.xyz/2022/05/03/h9R6X.png)

**Batch size**æ˜¯æˆ‘ä»¬è¦å†³å®šçš„è¶…å‚æ•°ã€‚å¦‚ä½•ç¡®å®šä¸¤è€…å¹³è¡¡ï¼ˆé±¼ä¸ç†ŠæŒï¼‰å‘¢ï¼Ÿï¼ˆæä¾›ä»¥ä¸‹é˜…è¯»èµ„æ–™å¯ä¾›å­¦ä¹ å‚è€ƒï¼‰

**==å…³äºMomentum==**

updateæ—¶æœ‰ä¸€ä¸ªâ€œåŠ¨é‡â€æˆ–æƒ¯æ€§ï¼Œä½¿å¾—æ¥è¿‘critical pointæ—¶ï¼Œä¸é™·å…¥å…¶ä¸­ï¼Œå¯ä»¥ç»§ç»­updateã€‚ï¼ˆä¸ä¸€å®šä¼šè¢«å¡ä½ï¼‰

* ä¸€èˆ¬çš„Gradient Descentï¼Œå›é¡¾_Lecture 1_
*   **Gradient Descent + Momentum**

    æ¯æ¬¡ç§»åŠ¨ï¼šä¸åªå¾€gradientåæ–¹å‘ç§»åŠ¨ï¼ŒåŒæ—¶åŠ ä¸Šå‰ä¸€æ­¥ç§»åŠ¨çš„æ–¹å‘ï¼Œä»è€Œè°ƒæ•´æ„æˆæˆ‘ä»¬çš„å‚æ•°ã€‚

    ![](https://s1.328888.xyz/2022/05/03/h9TLZ.png)

    $m^i$æ˜¯æ‰€æœ‰ä¹‹å‰æ¢¯åº¦åºåˆ—${g^0,g^1,...,g^{i-1\}}$çš„åŠ æƒå’Œã€‚

**æ€»ç»“ä¸€ä¸‹ä¸Šä¸¤èŠ‚æ‰€å­¦ï¼š**

* critical pointsè¡¨æ˜è¯¥å¤„æ¢¯åº¦ä¸º0
* critical pointå¯èƒ½æ˜¯saddle pointæˆ–æ˜¯local minimaï¼šå–å†³äºHessian matrixï¼›é€šè¿‡Hessian matrixçš„ç‰¹å¾å‘é‡æˆ‘ä»¬å¯ä»¥åœ¨æ¢¯åº¦ä¸º0çš„ç‚¹é‡æ–°æ›´æ–°æ–¹å‘ï¼›å¦å¤–ï¼Œlocal minimaå¯èƒ½å¹¶ä¸å¸¸è§
* Smaller batch sizeä»¥åŠmomentumå¯ä»¥å¸®åŠ©é€ƒå¼€critical pointsã€‚

***

**Tips for Training: Adaptive Learning Rate:**

å¼•å…¥ï¼šTraining Stuck $\neq$ Small Gradientã€‚ ä»¥ä¸‹å›¾ä¸ºä¾‹ï¼Œupdateåå¹¶æ²¡æœ‰å¡åœ¨critical pointï¼Œè€Œæ˜¯åœ¨ä¸¤ä¸ªç­‰é«˜ä½ç½®â€œåå¤æ¨ªè·³â€ï¼Œgradientä»»ç„¶å¾ˆå¤§ï¼Œè€Œlossæ— æ³•ä¸‹é™ã€‚

![](https://s1.328888.xyz/2022/05/03/h9cCC.png)

ä¸€èˆ¬çš„gradient descentçš„æ–¹æ³•ä¸‹ï¼Œåœ¨åˆ°è¾¾critical pointä¹‹å‰trainå°±åœæ­¢äº†ã€‚æ‰€ä»¥åœ¨å®åšä¸­å‡ºç°çš„é—®é¢˜å¾€å¾€ä¸åº”è¯¥æ€ªç½ªcritical pointã€‚

ç”±äºLearning Rate(LR:å­¦ä¹ ç‡)å†³å®šæ¯æ¬¡updateçš„æ­¥ä¼å¤§å°ï¼Œä»¥ä¸‹å›¾error surfaceä¸ºä¾‹ï¼ˆç›®æ ‡local minimaå³æ˜¯å›¾ä¸­æ©˜è‰²å°å‰å‰ï¼‰learning rateè¿‡å¤§ï¼Œtrainæ—¶ä¸€ç›´åœ¨ä¸¤è¾¹éœ‡è¡ï¼Œlossä¸‹é™ä¸ä¸‹å»ï¼›å½“learing rateè¾ƒå°æ—¶ï¼Œåœ¨æ¢¯åº¦è¾ƒå°çš„åœ°å¸¦ï¼Œæ— æ³•å¾—åˆ°æœ‰æ•ˆupdateï¼ˆèµ°ä¸è¿‡å»äº†...ï¼‰

![](https://s1.328888.xyz/2022/05/03/h9mbg.png)

ä»¥ä¸Šè¯´æ˜å­¦ä¹ ç‡ï¼ˆLearning Rateï¼‰ä¸èƒ½å¤Ÿ**one-size-fits-all**ã€‚åº”è¯¥æ˜¯ï¼Œå­¦ä¹ ç‡åº”å½“ä¸ºæ¯ä¸ªå‚æ•°**å®¢è´¨åŒ–**ã€‚â€”â€”Different parameters need different learning rate

åŸæ¥çš„ï¼š$\theta^{t+1}_i\leftarrow\theta^t\_i-\eta g^t\_i,g^t\_i=\frac{\partial L}{\partial \theta\_i}|_{\theta=\theta^t}$ï¼Œæ”¹è¿›åï¼š$\theta^{t+1}\_i \leftarrow \theta^t\_i - \large \frac{\eta}{\sigma^t\_i}g^t\_i$ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä»$\eta$æ”¹è¿›ä¸º$\Large\frac{\eta}{\sigma^t\_i}$ï¼Œåˆ†å·ä¸‹çš„$\sigma^t\_i$ï¼šå…¶ä¸­ä¸åŒçš„å‚æ•°ç»™å‡ºä¸åŒçš„$\sigma$ï¼ŒåŒæ—¶ä¸åŒçš„iterationç»™å‡ºä¸åŒçš„$\sigma$ï¼Œä»¥ä¸Šä¾¿æ˜¯parameter dependentçš„learning rateã€‚

\==**Root Mean Squareï¼š**$\sigma$== $\theta^{t+1}\_i \leftarrow \theta^t\_i - \large \frac{\eta}{\sigma^t\_i}g^t\_i$ï¼Œä»¥ä¸‹ä»‹ç»å¦‚ä½•è®¡ç®—$\sigma$

1. ç¬¬ä¸€æ­¥ï¼Œå½“$t=0$æ—¶ï¼Œ$\theta^{1}\_i \leftarrow \theta^0\_i - \large \frac{\eta}{\sigma^0\_i}\$$g^0\_i$ï¼Œ$\sigma^0\_i = \sqrt{(g^0\_i)^2} = |g^0\_i|$
2. ç¬¬äºŒæ­¥ï¼Œå½“$t=1$æ—¶ï¼Œ$\theta^{2}\_i \leftarrow \theta^1\_i - \large \frac{\eta}{\sigma^1\_i}\$$g^1\_i$ï¼Œ$\sigma^1\_i = \sqrt{\frac{1}{2}\[(g^0\_i)^2+(g^1\_i)^2]}$
3. å¦‚ä¸Šå½’çº³ï¼Œ$\sigma^t\_i = \sqrt{\frac{1}{t+1}\[(g^0\_i)^2+(g^1\_i)^2+...+(g^{t-1}\_i)^2+(g^t\_i)^2]}$

**Adagrad**ç®—æ³•ï¼š$\theta^{t+1}_i \leftarrow \theta^t\_i - \large \frac{\eta}{\sigma^t\_i}g^t\_i$ ä¸” $\sigma^t\_i=\sqrt{\frac{1}{t+1}\sum\limits_{t=0}^{t}(g^t\_i)^2}$

> [Deep Learning æœ€ä¼˜åŒ–æ–¹æ³•ä¹‹AdaGrad - çŸ¥ä¹ (zhihu.com)](https://zhuanlan.zhihu.com/p/29920135)

å½“æ¢¯åº¦å°æ—¶ï¼Œæ ¹æ®Adagradç®—æ³•ï¼Œ$\sigma$å°±å°ï¼Œå¯¼è‡´LRè¾ƒå¤§ï¼›åä¹‹ï¼Œæ¢¯åº¦å¤§ï¼Œ$\sigma$å¤§ï¼ŒLRå°ã€‚ä¸è¿‡å®ƒçš„ç¼ºç‚¹åœ¨äºå¯¹äºç¨ å¯†çš„updateä¸‹ï¼Œä¸æ–­åœ°å åŠ æ¢¯åº¦å¹³æ–¹å’Œä½¿å¾—$\sigma$å¿«é€Ÿå¢å¤§è€ŒLRéšä¹‹å¿«é€Ÿè¶‹äº0

![](https://s1.328888.xyz/2022/05/03/h93u1.png)

å¦å¤–ï¼Œå¯¹äºå…·ä½“é—®é¢˜ä¸‹å°±ç®—å¯¹äºåŒä¸€ä¸ªå‚æ•°ï¼ŒåŒä¸€ä¸ªæ›´æ–°æ–¹å‘ï¼ŒLRä¹Ÿè¢«æœŸæœ›å¯ä»¥åŠ¨æ€è°ƒæ•´â€”â€”**RMSProp**ç®—æ³•ï¼Œæ¥è‡ªHintonåœ¨Courseraçš„æˆè¯¾ï¼ˆæ²¡æœ‰è®ºæ–‡å¯å¼•ï¼‰

1. ç¬¬ä¸€æ­¥ï¼Œå½“$t=0$æ—¶ï¼Œ$\theta^{1}\_i \leftarrow \theta^0\_i - \large \frac{\eta}{\sigma^0\_i}\$$g^0\_i$ï¼Œ$\sigma^0\_i = \sqrt{(g^0\_i)^2} = |g^0\_i|$
2. ç¬¬äºŒæ­¥ï¼Œå½“$t=1$æ—¶ï¼Œ$\theta^{2}\_i \leftarrow \theta^1\_i - \large \frac{\eta}{\sigma^1\_i}\$$g^1\_i$ï¼Œ$\sigma^1\_i = \sqrt{\alpha(\sigma^0\_i)^2+(1-\alpha)(g^1\_i)^2},0<\alpha<1$
3. å¦‚ä¸Šå½’çº³ï¼Œ$\theta^{t+1}\_i \leftarrow \theta^t\_i - \large \frac{\eta}{\sigma^t\_i}g^t\_i$æ—¶ï¼Œ$\sigma^t\_i = \sqrt{\alpha(\sigma^{t-1}\_i)^2+(1-\alpha)(g^t\_i)^2},0<\alpha<1$

é€šè¿‡$\alpha$è¿™ä¸€é¡¹ï¼Œå¯ä»¥åŠ¨æ€è°ƒæ•´å¹³è¡¡æ¢¯åº¦å’Œå‰ä¸€æ­¥$\sigma$çš„å½±å“

![](https://s1.328888.xyz/2022/05/03/h9Kdt.png)

\==ç›®å‰ï¼Œæˆ‘ä»¬æœ€å¸¸ç”¨çš„åŠ¨æ€è°ƒæ•´LRçš„ç®—æ³•å°±æ˜¯**Adam**ï¼šRMSProp + Momentum==æ¨èé˜…è¯»å½•å…¥ICLR2015çš„[Adamæ–‡çŒ®](https://arxiv.org/pdf/1412.6980.pdf)ã€‚ç›¸å…³ç®—æ³•å·²ç»å†™å…¥pytorché‡Œäº†ï¼ˆè°ƒåŒ…å­xdmï¼‰

![](https://s1.328888.xyz/2022/05/03/h9OKe.png)

äº‹å®ä¸Šåœ¨å®é™…æ“ä½œæ—¶ï¼ŒLRå¹¶ä¸åƒæˆ‘ä»¬é¢„æœŸé‚£æ ·å¾ˆé¡ºåˆ©çš„åˆ°è¾¾local minimaï¼Œè€Œæ˜¯åœ¨æ¢¯åº¦è¾ƒå°çš„åœ°æ®µå‘ç”Ÿå‘å·¦å³ä¸¤è¾¹â€œäº•å–·â€çš„ç°è±¡ï¼ˆåŸå› æ²¡æ€ä¹ˆå¬æ‡‚ï¼‰ï¼Œå› æ­¤åšå‡ºä»¥ä¸‹ä¼˜åŒ–ï¼š

_**Learning Rate Scheduling**_ï¼š$\eta^t$

* **Learning Rate Decay** $\theta^{t+1}\_i \leftarrow \theta^t\_i - \large \frac{\eta^t}{\sigma^t\_i}g^t\_i$ï¼Œå³è®©$\eta$å’Œ$\sigma$ä¸€åŒå˜åŒ–
*   **Warm Up** â€œé»‘ç§‘æŠ€â€â€”â€”æ€»çš„æ¥è¯´ï¼š**LRå…ˆå˜å¤§åå˜å°**ï¼ˆè‡³äºè¦å˜åˆ°å¤šå¤§ä»¥åŠå˜åŒ–çš„é€Ÿç‡\[è¶…å‚æ•°]ä¹Ÿæ˜¯éœ€è¦è°ƒçš„ï¼‰DeepLearningè¿œå¤æ—¶æœŸçš„æ–‡ç« å°±æœ‰Warm Upäº†ï¼Œä¾‹å¦‚[Residual Network](https://arxiv.org/abs/1512.03385)ã€è¿™ç¯‡æ–‡ç« LRåˆå§‹è®¾0.01ä¹‹åè®¾0.1ã€‘ã€ä»¥åŠ[Transformer](https://arxiv.org/abs/1706.03762)

    ä¸ºä»€ä¹ˆä½¿ç”¨Warm Upä¼šæœ‰å¥½ä¸€äº›æ¶çš„è®­ç»ƒæ•ˆæœï¼Ÿç›®å‰ä¸ºæ­¢æ²¡æœ‰ä¸€ä¸ªå®Œç¾çš„è§£ç­”ã€‚æœ‰ä¸€ä¸ªè§£é‡Šæ˜¯ï¼šç”±äº$\sigma$åœ¨Adagradæˆ–æ˜¯Adamä¸­è¡¨ç°å‡ºçš„ä¸»è¦æ˜¯ç»Ÿè®¡æ„ä¹‰ï¼Œæ‰€ä»¥åœ¨åˆå§‹æ—¶æœŸå…¶ç›¸å…³ç»Ÿè®¡çš„æ•°æ®ä¸å¤Ÿå¤šæ—¶ï¼Œå…ˆè®©å…¶ä¸è¦è¿‡äºè¿œç¦»åˆå§‹ç‚¹ï¼Œæ¢ç´¢è·å–æ›´å¤šçš„æƒ…æŠ¥â€”â€”åˆ°åæœŸç´¯è®¡çš„æ•°æ®æ¯”è¾ƒå¤šï¼Œæ‰€ä»¥å¯ä»¥LRå¤§ä¸€äº›ã€‚[RAdam](https://arxiv.org/abs/1908.03265)æœ‰ç›¸å…³æ›´æ·±å…¥çš„è®¨è®ºã€‚

_**LRä¼˜åŒ–æ–¹æ³•çš„æ€»ç»“**_

Root Mean Squareï¼ˆRMSï¼‰ï¼š$\sigma$ åªè€ƒè™‘äº†æ¢¯åº¦çš„å¤§å°ï¼Œå¿½ç•¥äº†æ–¹å‘ï¼›è€ŒMomentumï¼š$m\_i^t$è¿˜è€ƒè™‘åˆ°äº†æ¢¯åº¦çš„æ–¹å‘.ã€‚æ€»çš„æ¥è¯´ï¼Œmomentumè¡¨è¾¾äº†å†å²è¿åŠ¨çš„æƒ¯æ€§ï¼Œè€ŒRMSåˆ™è‡´åŠ›äºå°†æ¢¯åº¦ä¸‹é™è¶‹äºå¹³ç¼“ã€‚

![](https://s1.328888.xyz/2022/05/03/h9aEO.png)

è¿™èŠ‚ä¸»è¦æ¢è®¨äº†åœ¨Error Surfaceå‘å‘æ´¼æ´¼çŠ¶æ€ä¸‹ï¼Œå¦‚ä½•è¾¾æˆæœ‰æ•ˆä¼˜åŒ–ã€‚ä¸‹ä¸€èŠ‚åˆ™è®²æˆå¦‚ä½•ä¼˜åŒ–Error Surfaceï¼ˆè§£å†³é—®é¢˜çš„æºå¤´ï¼Ÿï¼Ÿï¼‰,ä½¿å…¶å¹³æ»‘ã€‚

***

**Batch Normalizationï¼ˆQuick Introductionï¼‰**

ç®€çŸ­ä»‹ç»Batch Normalizationï¼Œä»¥åŠä¸€äº›tips$==\Rightarrow$æ‰¾åˆ°ä¸€ä¸ªæ»¡æ„çš„Error Sureface

ç”±äºè®­ç»ƒä¸­$x$å–å€¼å˜åŒ–å¾ˆå¤§ï¼Œæ‰€ä»¥å¯¼è‡´æ–œç‡å˜åŒ–â€å¤šç«¯â€œï¼Œåå·®å¾ˆå¤§ï¼Œäºæ˜¯ä½¿ç”¨å›ºå®šçš„LRè®­ç»ƒæ•ˆæœå¾ˆå·®ï¼Œä¸Šä¸€èŠ‚æ¢è®¨äº†å¦‚ä½•ç”¨ä¼˜åŒ–ï¼šåŠ¨æ€è°ƒæ•´LRã€‚è¿™é‡Œä»‹ç»ä¸‹è°ƒæ•´rangeçš„æ–¹æ³•ï¼š

*   \*Feature Normalizationï¼š\*å‡è®¾$x^1,x^2,x^3,...,x^r,...,x^R$ï¼šæ‰€æœ‰è®­ç»ƒé›†çš„Fearure Vector

    ![](https://s1.328888.xyz/2022/05/03/h91RP.png)

    æˆ‘ä»¬æŠŠä¸åŒvectorä¸‹çš„åŒä¸€ä¸ªdimensioné‡Œé¢çš„æ•°å€¼å»åšä¸€ä¸ª**å¹³å‡**$m\_i$ï¼Œå†åšä¸€ä¸ª**æ ‡å‡†å·®(standard deviation**ï¼‰è®°ä¸º$\sigma\_i$ï¼Œè¿™é‡Œå°±å¯ä»¥åšä¸€ä¸ª**æ ‡å‡†åŒ–ï¼ˆStandardizationï¼‰**ï¼š$\tilde{x}^r\_i \leftarrow \Large \frac{x^r\_i-m\_i}{\sigma\_i}$ã€‚å¥½å¤„ï¼šåŒä¸€ä¸ªdimensionä¸Šå¹³å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1ã€‚åœ¨deeplearningé‡Œï¼Œï¼ˆå°tipï¼‰æˆ‘ä»¬å¯ä»¥å¯¹ç‰¹å¾è¡ŒåšNormalizationï¼ˆå³Standardizationï¼‰ï¼Œè¿™ä¸ªæ“ä½œåœ¨æ¿€æ´»å‡½æ•°å‰æˆ–åéƒ½å¯ä»¥ï¼Œå®æˆ˜ä¸Šå·®åˆ«ä¸å¤§ã€‚

    ![](https://s1.328888.xyz/2022/05/03/hi3oB.png)

    Feature Normalizationå¯¼è‡´ç‹¬ç«‹è¾“å…¥çš„åˆå§‹inputç›¸äº’å…³è”èµ·æ¥ï¼Œå³åé¢çš„è¾“å‡ºå’Œå‰é¢çš„æ‰€æœ‰inputéƒ½æœ‰å…³ç³»ï¼ˆå› ä¸ºinputå…±åŒå†³å®šå‡å€¼å’Œæ–¹å·®ï¼‰ã€‚æœ‰ä¸€æ¡å¼¹å¹•ï¼šbatchå†…éƒ¨æ¯éš”sampleäº’ç›¸å…³ï¼Œbatchå’Œbatchä¹‹é—´ç›¸äº’ç‹¬ç«‹ã€‚

    å®æˆ˜ä¸­ï¼Œè€ƒè™‘åˆ°GPUçš„å®é™…å†…å­˜ï¼Œæˆ‘ä»¬ä¸€èˆ¬åœ¨ä¸€ä¸ªbatchä¸ŠåšFeature Normalizationï¼Œæ‰€ä»¥è¿™æ‹›ä¹Ÿå«**Batch Normalization**ã€‚å½“ç„¶è¿™ä¼šå¯¼è‡´batchä¹‹é—´çš„å¼‚è´¨æ€§ã€‚

    å¦å¤–ï¼Œç»éªŒä¹‹è°ˆï¼Œ$\tilde{x}^i = \gamma \odot \tilde{x}^i+\beta$ï¼ˆåˆå§‹æ—¶$\gamma$ä¸ºå•ä½å‘é‡ï¼Œ$\beta$ä¸ºé›¶å‘é‡ï¼‰ã€‚pyTorchåœ¨ç®—Batch Normalizationæ—¶ä¼šæŠŠ$\mu$å’Œ$\sigma$æ‹¿å‡ºæ¥åšmoving averageã€‚

    Batch Normalizationç”¨åœ¨CNNä¸Šï¼Œè®­ç»ƒé€Ÿåº¦ä¼šå˜å¿«ã€‚

    è¿™æ˜¯ä¸€ä¸ªserendipitousï¼ˆæœºç¼˜å·§åˆçš„ï¼‰discovery
* Layer Normalization
* Instance Normalization
* Group Normalization
* Weight Normalization
* Spectrum Normalization

#### Lecture 2\*\*ï¼šåˆ†ç±»ï¼ˆClassificationï¼‰BRIEFç‰ˆ

* Regressionï¼š$x \Rightarrow$ **model** $\Rightarrow y \Leftarrow \Rightarrow \hat{y}$
* Classificationï¼šå¥‡å¦™çš„æ–¹æ³•---æŠŠåˆ†ç±»å½“ä½œå›å½’

_**Class as one-hot vector**_ï¼Œä¸¾ä¾‹ï¼šæ¯ä¸ªç±»ä½œä¸ºä¸€ä¸ª **one-hot vector**![](https://s1.328888.xyz/2022/05/12/HoOSQ.png)

![](https://s1.328888.xyz/2022/05/03/h9VXA.png)

classificationé‡Œçš„yæ˜¯ä¸€ä¸ªå‘é‡ï¼ˆè€Œéæ•°å€¼ï¼‰ï¼Œå¦å¤–ä¸Regressionä¸åŒçš„æ˜¯ï¼Œ$y' = softmax(y)$ã€‚$softmax$çš„ä½œç”¨æ˜¯å°†$y$å€¼æ˜ å°„åˆ°$\[0,1]$é‡Œï¼Œå…¶åŸç†åŸå› è‡ªè¡Œæ¢è®¨ã€‚

![](https://s1.328888.xyz/2022/05/03/h9ZCS.png)

é™¤äº†æ­£åˆ™åŒ–çš„æ•ˆæœå¤–ï¼Œ**softmax**è¿˜å¯ä»¥è®©å¤§å€¼å’Œå°å€¼å·®è·æ›´å¤§ã€‚å½“åªæœ‰ä¸¤ä¸ªclassæ—¶ï¼Œå°±ç›´æ¥ç”¨$sigmoid$äº†ï¼Œæˆ‘ä»¬å¯ä»¥è®¤ä¸º$softmax$æ˜¯$sigmoid$çš„æ‰©å±•ï¼Œå¯ä»¥ç”¨åœ¨ä¸‰ä¸ªåŠä»¥ä¸Šclassçš„æƒ…å½¢ã€‚

*   **Loss of Classification** $L = \frac{1}{N}\sum\limits\_{i}e\_n$ï¼Œä»¥ä¸‹ä»‹ç»äº†MSEå’Œäº¤å‰ç†µ

    ![](https://s1.328888.xyz/2022/05/03/h9jbR.png)

    è¿™é‡Œäº¤å‰ç†µï¼ˆCross-entropyï¼‰æ›´ä¼˜ï¼Œ**äº¤å‰ç†µæœ€å°ï¼ˆMinimizing cross-entropyï¼‰ç­‰ä»·äºæœ€å¤§ä¼¼ç„¶ï¼ˆMaximizing likelihoodï¼‰**

    äº¤å‰ç†µå’Œ**softmax**åœ¨ä½¿ç”¨æ—¶é€šå¸¸ç»‘å®šåœ¨ä¸€èµ·ï¼ˆpytorchçš„è®¾è®¡å¦‚æ­¤ï¼‰

    ç›¸æ¯”äºMSEï¼Œcross-entropyæ›´è¢«å¸¸ä½¿ç”¨åœ¨åˆ†ç±»ä»»åŠ¡ä¸Šï¼Œä»¥ä¸‹ä»Optimizationçš„è§’åº¦çš„è§£é‡ŠğŸ‘‡

    ![](https://s1.328888.xyz/2022/05/03/h96yi.png)

    è¿™é‡Œ$e$å¯èƒ½æ˜¯MSEæˆ–cross-entropyã€‚

    ![](https://s1.328888.xyz/2022/05/03/h9Mdv.png)

    ä¸¤è€…çš„ä»»åŠ¡éƒ½æ˜¯ä»å·¦ä¸Šè§’ä¸€è·¯åˆ°å³ä¸‹è§’ï¼Œä½†æ˜¯åœ¨MSEä¸Šï¼Œlosså¾ˆå¤§çš„åœ°æ–¹éå¸¸å¹³å¦ï¼ˆæ¢¯åº¦å°ï¼‰ï¼Œå¾ˆå®¹æ˜“è¢«stuckèµ°ä¸ä¸‹å»ï¼›è€Œcross-entropyåˆ™ç›¸æ¯”èµ·æ¥å¥½å¾ˆå¤šã€‚
