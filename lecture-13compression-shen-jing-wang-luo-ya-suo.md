# Lecture 13ï¼šCompressionï¼šç¥ç»ç½‘ç»œå‹ç¼©

> Lectured by HUNG-YI LEE (æå®æ¯…)
>
> Recorded by Yusheng zhaoï¼ˆyszhao0717@gmail.comï¼‰

***

\[TOC]

***

> æˆ‘ä»¬è§è¿‡åƒBERTã€GPT-3è¿™æ ·çš„åºå¤§çš„æ¨¡å‹ï¼Œé‚£æˆ‘ä»¬èƒ½ä¸èƒ½ç®€åŒ–è¿™äº›æ¨¡å‹ï¼Œè®©ä»–ä»¬æœ‰è¾ƒå°‘çš„å‚æ•°ä½†æ˜¯è·ŸåŸæ¥çš„æ¨¡å‹æ•ˆèƒ½å·®ä¸å¤šã€‚â€”â€”Network Compression

åº”ç”¨åœºæ™¯ï¼šå°†MLæ¨¡å‹åº”ç”¨åœ¨èµ„æºæ¯”è¾ƒæœ‰é™çš„æƒ…å†µä¸‹è­¬å¦‚ç‰©è”ç½‘è®¾å¤‡æˆ–æ˜¯å°å‹ç”µå™¨ä¸Šï¼ˆä¾‹å¦‚æ™ºèƒ½æ‰‹è¡¨ã€æ— äººæœºç­‰ç­‰ï¼‰ï¼Œè¿™å°±è¦æ±‚ML modelä¸èƒ½å¤ªå¤§ï¼Œå› æ­¤æ¨¡å‹å‹ç¼©å¯¹ç®—æ³•åœ¨å·¥ä¸šåº”ç”¨è½åœ°æœ‰å…¶å¿…è¦æ€§ã€‚

è¿™ç±»åº”ç”¨åœºæ™¯å¾€å¾€éœ€è¦**ä½å»¶è¿Ÿï¼ˆlow latencyï¼‰**ã€ä»¥åŠ**éšç§ï¼ˆprivacyï¼‰ä¿æŠ¤**çš„éœ€æ±‚ã€‚

å› æ­¤æ— æ³•åº”ç”¨äº‘è®¡ç®—ï¼ˆè¾“å…¥é€šè¿‡æ— çº¿ç½‘ç»œä¼ å…¥äº‘ç«¯ï¼Œåœ¨äº‘ç«¯è®¡ç®—ï¼Œè¾“å‡ºä¼ å›æ¥ï¼‰â€”â€”å…¶ä¸€åŸå› æ˜¯äº‘è®¡ç®—å¸¦æ¥ä¿¡æ¯ä¼ é€’çš„å»¶è¿Ÿï¼ˆLatencyï¼‰ï¼Œä¸€äº›å°å‹ç‰©è”ç½‘è®¾å¤‡åº”ç”¨åœºæ™¯éœ€è¦å°½é‡å¿«é€Ÿçš„å®æ—¶ååº”ã€‚

å°½ç®¡5Gæ—¶ä»£æˆ–è®¸ä¼šè§£å†³ä¿¡æ¯ä¼ é€’çš„å»¶è¿Ÿï¼Œä½†æ˜¯å°†ç”¨æˆ·éšç§æ•°æ®å‚¨å­˜åœ¨äº‘ç«¯è¿›è¡Œè¿ç®—å»ºæ¨¡ï¼Œä¹Ÿæœ‰ç€å¯¹éšç§ä¾µçŠ¯çš„éšæ‚£ã€‚

> é’ˆå¯¹éšç§åœ¨äº‘ç«¯çš„ä¿æŠ¤ï¼Œæ¶‰åŠåˆ°äº‘ç«¯ä¸Šå»ºæ¨¡ä½¿ç”¨è”é‚¦å­¦ä¹ ã€æˆ–åœ¨éšç§æ•°æ®ä¸Šè¿›è¡ŒåŒæ€åŠ å¯†ã€‚æ­¤å¤„ä¸æ˜¯æœ¬lectureè®¨è®ºé‡ç‚¹ã€‚

å› æ­¤ï¼ŒNetwork Compressionæœ‰å…¶åº”ç”¨ä»·å€¼å’Œæ½œåŠ›ã€‚

> ä»¥ä¸‹ä»‹ç»äº”ç§ç¥ç»ç½‘ç»œå‹ç¼©çš„æŠ€æœ¯ï¼ˆå…¨æ˜¯è½¯ä»¶å±‚é¢ï¼‰ï¼Œæœ¬lectureä¸è€ƒè™‘ç¡¬ä»¶å±‚é¢çš„è¿ç®—åŠ é€Ÿæˆ–å‹ç¼©æ–¹æ¡ˆ

## Network Pruningï¼šç½‘ç»œå‰ªæ

æ‰€è°“æ ‘å¤§å¿…æœ‰æœ«æï¼Œå¤šæ•°çš„å¤§å‹ç¥ç»ç½‘ç»œé€šå¸¸éƒ½æ˜¯over-parameterizedï¼Œè¿™äº›æ²¡æœ‰ç”¨çš„â€œæ‘¸é±¼å‚æ•°â€ï¼ˆredundant weights or neuronsï¼‰å¯ä»¥è¢«å‰ªæ‰ï¼Œè€Œä¸å½±å“æ¨¡å‹æ•´ä½“çš„æ€§èƒ½ã€‚

90å¹´ä»£ï¼ŒYann LeCunå‘è¡¨çš„ä¸€ç¯‡â€Optimal Brain Damageâ€œæ¢ç´¢äº†å¦‚ä½•optimizeè„‘çš„ç¥ç»å…ƒè€Œä½¿è„‘çš„æŸä¼¤æœ€å°‘ï¼Œå’ŒNetwork Pruningæœ‰å¼‚æ›²åŒå·¥ä¹‹å¦™ã€‚

### Network Pruningçš„æµç¨‹

* é¦–å…ˆè®­ç»ƒä¸€ä¸ªåˆå§‹çš„ç¥ç»ç½‘ç»œï¼ˆLarge Pre-trained Networkï¼‰ï¼Œé€šå¸¸è¿™ä¸ªæ¨¡å‹çš„è§„æ¨¡å¾ˆå¤§ã€‚
* è¯„ä¼°è¿™ä¸ªå¤§çš„networké‡Œè¾¹çš„å‚æ•°ï¼Œæ‰¾æ‰¾é‡Œè¾¹å“ªäº›å‚æ•°æ˜¯åœ¨åšäº‹çš„ï¼ˆattach of importanceï¼‰ï¼Œå“ªäº›å‚æ•°æ˜¯åœ¨æ‘¸é±¼çš„ã€‚æœ€ç®€å•çš„çœ‹ä»å‚æ•°ç»å¯¹å€¼ï¼Œå¦‚æœç»å¯¹å€¼è¶Šå¤§è¯´æ˜å‚æ•°å¯¹networkå½±å“è¶Šå¤§ã€‚å¯¹äº**å‚æ•°**å’Œ**ç¥ç»å…ƒ**çš„é‡è¦æ€§çš„è¯„ä¼°å‚è€ƒå¦‚ä¸‹æ‰€ç¤ºï¼š
  * _Importance of a weight_ï¼šç»å¯¹å€¼ï¼ˆå¦‚æœæ¥è¿‘äº0è¯´æ˜å¯¹networkå½±å“ä¸å¤§ï¼‰ã€å¥—ç”¨lifelong learningçš„æƒ³æ³•ï¼šè®¡ç®—æ¯ä¸ªå‚æ•°çš„BIå€¼ï¼ˆå‚æ•°æ›´æ–°æƒé‡ï¼Œè¡¨ç¤ºå‚æ•°æœ‰å¤šé‡è¦ï¼Œèƒ½ä¸èƒ½å˜åŒ–å¤ªå¤šï¼‰
  * _Importance of a neuron_ï¼šè®¡ç®—åœ¨æ•°æ®é›†ä¸Šè®­ç»ƒè¿‡ç¨‹ä¸­ç¥ç»å…ƒè¾“å‡ºä¸ä¸º0çš„æ¬¡æ•°
* æŠŠä¸é‡è¦çš„å‚æ•°æˆ–ç¥ç»å…ƒä»æ¨¡å‹ä¸­ç§»é™¤ï¼Œè¿™æ ·å°±å¾—åˆ°è¾ƒå°è§„æ¨¡çš„ç¥ç»ç½‘ç»œ
* å‰ªæï¼ˆpruningï¼‰è¿‡åï¼Œé€šå¸¸æ¨¡å‹çš„å‡†ç¡®ç‡ï¼ˆaccuracyï¼‰ä¼šæ‰ä¸€ç‚¹ï¼›æŠŠæ²¡å‰ªæ‰çš„å‚æ•°åœ¨è®­ç»ƒèµ„æ–™ä¸Šè¿›è¡Œå¾®è°ƒï¼ˆfine-tunedï¼‰å¯ä»¥æ¢å¤æ¨¡å‹æ€§èƒ½
* é‡å¤ç¬¬äºŒæ­¥åˆ°ç¬¬å››æ­¥ï¼Œä¸æ–­å‰ªæï¼Œç›´åˆ°networkå¤Ÿå°ä¸”æ€§èƒ½å¤§å·®ä¸å·®ã€‚tipsï¼šå°å‰ªå¤šæ¬¡ï¼Œä¸è¦ä¸€æ¬¡æ€§å‰ªå¤ªå¤šï¼Œå¦åˆ™ç¥ç»ç½‘ç»œå¯èƒ½recoverä¸å›æ¥ã€‚

![image-20220417203928547](https://s1.328888.xyz/2022/05/04/hWMjd.png)

### Practical Issueï¼šç”¨å‚æ•°ï¼ˆweightï¼‰æˆ–ç¥ç»å…ƒï¼ˆneuronï¼‰å½“ä½œå‰ªæå•ä½çš„å·®åˆ«

#### Weight pruning

å½“æˆ‘ä»¬å»æ‰æŸä¸€ä¸ª/äº›ä¸é‡è¦çš„å‚æ•°åï¼Œæ¨¡å‹å½¢çŠ¶ä¼šå˜å¾—ä¸è§„åˆ™ï¼ˆå¦‚ä¸‹å›¾å³ï¼‰

![image-20220417204518112](https://s1.328888.xyz/2022/05/04/hWoW3.png)

å½¢çŠ¶â€œä¸è§„åˆ™â€çš„æ¨¡å‹æ˜¯**ä¸å¥½å®åšçš„**ï¼ˆcodingå¾ˆä¸æ–¹ä¾¿â€¦ï¼‰è€Œä¸”**ä¸åˆ©äºç”¨GPUåšåŠ é€Ÿè¿ç®—**ã€‚å¦‚æœéè¦åšï¼Œåªèƒ½æŠŠå»æ‰çš„weightè¡¥ä¸º0ï¼Œè¿™åˆè¿åäº†æˆ‘ä»¬åšå‰ªæçš„åˆè¡·ï¼Œå­˜å‚¨ä¸­ä¾ç„¶å­˜åœ¨weightï¼ˆå³ä¾¿æ˜¯0ï¼‰ï¼Œæ¨¡å‹æ ¹æœ¬æ²¡æœ‰å˜å°â€¦remarkï¼š**hard to implement and speedup**

æ¥è‡ªhttps://arxiv.org/pdf/1608.03665.pdfçš„å®éªŒï¼›éªŒè¯äº†å³ä¾¿å‰ªææ‰ä¹æˆå¤šçš„weightï¼Œä½†æ˜¯speedupçš„æ•ˆæœä¾ç„¶å¾ˆå·®ã€‚

![image-20220417205308171](https://s1.328888.xyz/2022/05/04/hWSJQ.png)

> å¦‚æœè‡ªå·±å†™ä¸ªåº“ç”¨äºweight pruningçš„å®ç°å’ŒåŠ é€Ÿâ€¦â€¦

#### Neuron pruning

å½“æˆ‘ä»¬å»æ‰æŸä¸€ä¸ª/äº›ä¸é‡è¦çš„ç¥ç»å…ƒåï¼Œæ¨¡å‹å½¢çŠ¶ä¾ç„¶ä¿æŒè§„åˆ™ï¼ˆregularï¼‰ï¼Œå¦‚ä¸‹å›¾å³

![image-20220417205529325](https://s1.328888.xyz/2022/05/04/hWBn4.png)

è¿™æ—¶å€™ç”¨Pytorchæ¯”è¾ƒæ–¹ä¾¿å®ç°ï¼Œæ”¹æ”¹è¾“å…¥è¾“å‡ºçš„dimensionå°±è¡Œï¼›ä¹Ÿæ¯”è¾ƒåˆ©äºGPUåŠ é€Ÿã€‚**Easy to implement and speedup !**

### Why Pruning ï¼Ÿ

$Q\_1$ï¼šä¸ºä»€ä¹ˆä¸ç›´æ¥trainä¸€ä¸ªå°çš„networkï¼Ÿ

$Ans$ï¼šå¤§çš„networkæ¯”è¾ƒå¥½trainï¼ˆå®è·µéªŒè¯ï¼Œä¼—æ‰€å‘¨çŸ¥ï¼‰ï¼›ç„¶åå†æŠŠå¤§æ¨¡å‹ç£¨å°ã€‚å¦‚æœç›´æ¥trainå°çš„networkéš¾ä»¥è¾¾åˆ°å¤§æ¨¡å‹å†pruningåå¾—åˆ°çš„å°æ¨¡å‹çš„æ€§èƒ½

*   ä¸ºä»€ä¹ˆå¤§æ¨¡å‹æ¯”è¾ƒå¥½trainå‘¢ï¼Ÿ

    \*\*\*ä¹é€å½©ç¥¨å‡è¯´ï¼ˆLottery Ticket Hypothesisï¼‰\*\*\*https://arxiv.org/abs/1803.03635

    ![image-20220417211130912](https://s1.328888.xyz/2022/05/04/hWPUB.png)

    æ­£å¦‚ç‚¼ä¸¹å¾ˆç„å­¦ä¸€æ ·ï¼ˆçœ‹äººå“ï¼‰â€¦â€¦è¦æƒ³ä¸­å½©ç¥¨ï¼ˆè·å¾—å¾ˆåˆé€‚çš„ä¸€ç»„å‚æ•°ï¼‰ï¼Œå°±è¦å¤§é‡çš„ä¹°å½©ç¥¨ï¼›ä¸€å‡»å³ä¸­å¾ˆå—çš„å•¦ã€‚

    æˆ‘ä»¬å¯ä»¥æŠŠå¤§çš„networkåˆ†ä¸ºå„ä¸ªsub-networkï¼Œæ¯ä¸ªsub-networkéƒ½æœ‰ä¸€å®šæ¦‚ç‡æˆåŠŸï¼ˆè·å¾—å¾ˆåˆé€‚çš„ä¸€ç»„å‚æ•°ï¼‰æˆ–å¤±è´¥ï¼›å¯¹äºlarge networkï¼Œåªè¦å…¶ä¸­ä¹‹ä¸€å­ç½‘ç»œè®­ç»ƒå¾ˆæˆåŠŸï¼Œå°±èƒ½è¾¾åˆ°â€œä¸€äººå¾—é“ï¼Œé¸¡çŠ¬å‡å¤©â€çš„æ•ˆæœâ€”â€”å¤§çš„networkå°±æˆåŠŸäº†ï¼ˆwinï¼ï¼‰ã€‚

    * _å®éªŒ_ï¼šéªŒè¯ä¹é€å‡è¯´ï¼ˆå’Œnetwork pruningå…³ç³»å¯†åˆ‡ï¼‰
      * å…ˆå¯¹large networkåšå‚æ•°çš„éšæœºåˆå§‹åŒ–ï¼›ç„¶åtrainåå¾—åˆ°ä¸€ç»„è®­ç»ƒå®Œçš„å‚æ•°ï¼›å†åšnetwork pruningå¾—åˆ°ä¸€ä¸ªè¾ƒå°çš„network
      *   å†æŠŠè¾ƒå°çš„networkçš„å‚æ•°éšæœºåˆå§‹åŒ–ï¼Œå†trainå‘ç°trainä¸èµ·æ¥ï¼›ä½†æ˜¯ï¼Œå¦‚æœæŠŠä¸€å¼€å§‹å¤§æ¨¡å‹çš„éšæœºåŒ–çš„å‚æ•°å¯¹åº”çš„èµ‹ç»™å°æ¨¡å‹ï¼Œå†trainï¼Œèƒ½trainèµ·æ¥ã€‚

          ![image-20220417212224572](https://s1.328888.xyz/2022/05/04/hWxqT.png)

    å°è¯•è§£æ„ä¹é€å‡è¯´çš„æ–‡ç« [Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask](https://arxiv.org/abs/1905.01067)â€”â€”å°è¯•äº†ä¸åŒçš„pruningçš„ç­–ç•¥ï¼Œå‘ç°è¯´æŸä¸¤ä¸ªç­–ç•¥æ˜¯æœ€æœ‰æ•ˆçš„ï¼Œå¦‚æœtrainå‰åç»å¯¹å€¼å·®è·è¶Šå¤§ï¼Œpruningæ‰çš„networkç»“æœæ˜¯æœ€æœ‰æ•ˆçš„ã€‚

    ![image-20220417212758688](https://s1.328888.xyz/2022/05/04/hzQw2.png)

    å…¶ä»–ä¸€äº›æœ‰è¶£çš„ç»“è®ºï¼š

    *   åˆå§‹åŒ–å‚æ•°æ­£è´Ÿå·ç¬¦å·å¾ˆé‡è¦ï¼ˆcriticalï¼‰ã€‚

        ä¸€ç»„å¥½çš„åˆå§‹åŒ–å‚æ•°åˆ°åº•æ˜¯å¥½åœ¨å“ªé‡Œå‘¢ï¼Ÿå®éªŒå‘ç°å°çš„networkå¦‚æœä¸æ”¹å˜original random initçš„å‚æ•°çš„æ­£è´Ÿå·ä¹Ÿæ˜¯èƒ½trainçš„èµ·æ¥çš„ã€‚è¿™è¯´æ˜å‚æ•°çš„ç»å¯¹å€¼ä¸é‡è¦ï¼Œå‚æ•°çš„æ­£è´Ÿæ‰é‡è¦ã€‚
    *   Pruning weights from a network with random weights

        ä¸€ä¸ªåˆå§‹éšæœºåŒ–å‚æ•°çš„å¤§æ¨¡å‹å·²ç»æœ‰ä¸€ä¸ªsub-networkï¼Œå®ƒçš„åˆå§‹å‚æ•°åˆšå¥½éšæœºåŒ–çš„å¾ˆåˆé€‚ï¼Œå¯ä»¥ç›´æ¥æ‹¿æ¥ç”¨åœ¨ä»»åŠ¡ä¸­ï¼Œåœ¨å®éªŒä¸­çš„åˆ†ç±»ä»»åŠ¡ä¸­å¾—åˆ°å’Œç›‘ç£å­¦ä¹ å¾ˆæ¥è¿‘çš„æ­£ç¡®ç‡ã€‚ï¼ˆæ­¤å¤„è€å¸ˆæ‰“äº†ä¸€ä¸ªæ°å½“çš„æ¯”æ–¹ï¼šç±³å¼€æœ—çªç½—é›•åˆ»å¤§å«ï¼Œä¸æ˜¯åˆ»æ„çš„é›•åˆ»è€Œæ˜¯æŠŠçŸ³å¤´ä¸­çš„å¤§å«å½¢è±¡å–äº†å‡ºæ¥ï¼‰

        ç±»ä¼¼çš„å‘ç°æ¥è‡ªæ–‡ç« [Weight Agnostic Neural Networks](https://arxiv.org/abs/1906.04358)ï¼Œæ–‡ç« ä¸­ç½‘ç»œéšæœºçš„å‚æ•°è¦ä¸æ˜¯éšæœºçš„è¦ä¸å°±ç»Ÿç»Ÿè®¾ä¸º1.5ï¼Œè¿™ä¸ªnetworkå¯ä»¥å¾—åˆ°ä¸€å®šçš„å¥½çš„performanceã€‚
* [**Rethinking the value of Network Pruning**](https://arxiv.org/abs/1810.05270)

ä¹é€å‡è¯´åŒæ—¶æœŸçš„å·¥ä½œï¼Œå¯¹äºpruningçš„ä¸€ç§è§‚ç‚¹ï¼Œæœ‰æ—¶é—´ç²¾è¯»ä¸‹è¿™ç¯‡è®ºæ–‡ã€‚

![image-20220417223019923](https://s1.328888.xyz/2022/05/04/hzUoM.png)

ä¸¤ä¸ªæ•°æ®é›†ï¼Œå››ä¸ªæ¨¡å‹ï¼Œåˆ†åˆ«è®¾unprunedå’Œprunedï¼Œåšä¸€ä¸‹fine-tunedï¼Œå¾—åˆ°æŒ‡æ ‡ã€‚é’ˆå¯¹pruned modelï¼šğŸ‘‡

ç¬¬ä¸€æ¬¡å®éªŒæ˜¯scratch-Eï¼ˆå¯¹äºå°æ¨¡å‹åˆå§‹å‚æ•°æ˜¯**çœŸçš„**éšæœºåˆå§‹åŒ–ï¼Œå½¼æ—¶ä¹é€å‡è¯´ä¸­å°æ¨¡å‹é‡æ–°trainå‰åˆå§‹åŒ–å‚æ•°æ¥æºäºprunedå‰çš„å¤§æ¨¡å‹ï¼‰ï¼Œå‘ç°æŒ‡æ ‡ç¡®å®ç•¥é€Šä¸€äº›

ç¬¬äºŒæ¬¡å®éªŒåœ¨ç¬¬ä¸€æ¬¡å®éªŒçš„åŸºç¡€ï¼Œpruned modelè®­ç»ƒæ—¶çš„epochå¤šåŠ å‡ ä¸ªï¼Œç»“æœå°±æ¯”fine-tunedå¥½ã€‚è¿™ç¯‡æ–‡ç« å’Œä¹é€å‡è¯´çš„æƒ³æ³•æ˜¯ç›¸åçš„ï¼Œé‡Œé¢ä¹Ÿæå‡ºäº†ä¹é€å‡è¯´çš„å¯èƒ½éšå«çš„æˆç«‹æ¡ä»¶ï¼šå½“learning rateè®¾çš„è¾ƒå°ã€è¿˜æœ‰unstructuredï¼ˆä»¥weightä½œä¸ºå•ä½æ¥pruningï¼‰çš„æ—¶å€™å¥½åƒæ‰ä¼šå‘ç°ä¹é€å‡è¯´çš„ç°è±¡ã€‚

ä¹é€å‡è¯´æ˜¯çœŸæ˜¯å‡ï¼Œéœ€è¦æ›´å¤šçš„ç ”ç©¶æ¥è¯å®ã€‚

## Knowledge Distillationï¼šçŸ¥è¯†è’¸é¦

> çŸ¥è¯†è’¸é¦çš„æ€æƒ³å’Œpruningæœ‰ä¸€äº›ç±»ä¼¼çš„åœ°æ–¹
>
> å‚è€ƒæ–‡çŒ®ï¼š[Knowledge Distillation](https://arxiv.org/pdf/1503.02531.pdf)ã€[Do Deep Nets Really Need to be Deep?](https://arxiv.org/pdf/1312.6184.pdf)æå‡ºäº†çŸ¥è¯†è’¸é¦æˆ–ç±»ä¼¼çš„æƒ³æ³•

æˆ‘ä»¬å…ˆtrainä¸€ä¸ªå¤§çš„networkï¼šç§°ä¹‹ä¸º**Teacher Net**ï¼ˆLargeï¼‰ï¼›æˆ‘ä»¬æœ€ç»ˆè¦å¾—åˆ°çš„å°çš„networkç§°ä¹‹ä¸º**Student Net**ã€‚

å’Œpruningä¸ä¸€æ ·çš„åœ°æ–¹æ˜¯ï¼Œpruningä¸­å°ç½‘ç»œæ˜¯å¤§ç½‘ç»œï¼ˆä¿®å»ºåï¼‰çš„è¡ç”Ÿå“ï¼›è€Œknowledge distillationä¸­Student netæ˜¯å»æ ¹æ®teacher netæ¥å­¦ä¹ ã€‚

![image-20220418105303936](https://s1.328888.xyz/2022/05/04/hzu27.png)

teacher netæŒ‰ç…§ä¸€èˆ¬ç¥ç»ç½‘ç»œæ¥è®­ç»ƒï¼ˆå¦‚å›¾ä»¥mniståšæ‰‹å†™æ•°å­—åˆ†ç±»å™¨ï¼‰ï¼›è€Œstudent netä»¥è€å¸ˆçš„embeddingï¼ˆä¸Šå›¾ä¸­æ˜¯\[0, 0.7,..., 0.2, 0, 0.1]ï¼Œåç»´çš„ï¼‰ä½œä¸ºå­¦ä¹ ç›®æ ‡è€Œéåˆ†ç±»æ­£ç¡®ç‡ï¼›å­¦ç”Ÿçš„ç›®æ ‡å°±æ˜¯å°½é‡é€¼è¿‘è€å¸ˆçš„è¾“å‡ºï¼ˆå°±ç®—è€å¸ˆæ˜¯é”™çš„ä¹Ÿç›´æ¥å­¦ï¼‰ï¼Œoptimizationçš„æ–¹æ³•ä¸ºcross-entropy minimizationã€‚

ç›®å‰æ¥çœ‹çŸ¥è¯†è’¸é¦çš„å¥½å¤„ï¼šå¯¹äºstudent netæ¥è¯´ï¼Œteacher netä¸ä»…å°½é‡æä¾›äº†æ­£ç¡®çš„åˆ†ç±»ç­”æ¡ˆï¼Œè€Œä¸”è¿˜æä¾›äº†é¢å¤–çš„ä¿¡æ¯ï¼ˆç›¸æ¯”ç›´æ¥å‘Šè¯‰ç½‘ç»œæ­£ç¡®åˆ†ç±»çš„æ ‡è®°ï¼‰ï¼šå¦‚ä¸Šå›¾ï¼Œâ€œ1â€ã€â€œ7â€å’Œâ€œ9â€é•¿çš„æœ‰ç‚¹åƒã€‚

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå…‰æ˜¯å‡­ç€teacher netçš„æ•™å­¦å“ªäº›æ•°å­—æœ‰æ€æ ·çš„å…³ç³»ï¼Œstudent netå¯èƒ½ç”šè‡³å¯ä»¥åœ¨æµ‹è¯•ä¸­è¯†åˆ«å‡ºè®­ç»ƒé›†ä¸­ç¼ºä¹çš„æ•°å­—ã€‚

*   Teacher networkä¸ä»…å¯ä»¥æ˜¯ä¸€ä¸ªå•ä¸€çš„ç¥ç»ç½‘ç»œï¼Œå®ƒä¹Ÿå¯ä»¥æ˜¯é›†æˆï¼ˆEnsembleï¼‰çš„ç¥ç»ç½‘ç»œï¼ˆN Networksï¼‰

    æ‰€è°“Ensemble Learningï¼šï¼ˆç®€ç•¥ä»‹ç»ä¸‹ï¼‰å°±æ˜¯è®­ç»ƒå¤šä¸ªæ¨¡å‹ï¼Œç„¶ååšä¸€ä¸ªå¹³å‡ï¼ˆAverage many modelsï¼‰ï¼›å°†å¹³å‡çš„ç»“æœä½œä¸ºæœ€åçš„ç­”æ¡ˆã€‚å®ç”¨ä¸Šè®²ï¼Œé›†æˆå­¦ä¹ æ—¶é—´å¼€é”€æ¯”è¾ƒå¤§ï¼Œåœ¨å·¥ä¸šä¸Šä¸å¥½ä½¿ç”¨ï¼Œç»å¸¸ä½¿ç”¨åœ¨æœºå™¨å­¦ä¹ æ¯”èµ›ä¸Šã€‚

    æˆ‘ä»¬æŠŠEnsembleå¹³å‡è¾“å‡ºä½œä¸ºteacher netçš„ç»“æœï¼Œç„¶åç”¨student netè®­ç»ƒé€¼è¿‘è¿™ä¸ªç»“æœï¼ˆåŒæ ·ç”¨åˆ°cross-entropy minimizationï¼‰
*   ä¸€ä¸ªå°trickï¼š

    Temperature for softmax

    åˆå§‹çš„softmaxå¦‚ä¸‹ï¼š$\large{ y\_i' = \frac{\exp(y\_i)}{\sum\_j\exp(y\_j)\}}$ï¼Œ$\Rightarrow$ åŠ äº†â€œæ¸©åº¦â€çš„softmaxï¼š$\large{ y\_i' = \frac{\exp(y\_i/T)}{\sum\_j\exp(y\_j/T)\}}$

    ![image-20220418112836480](https://s1.328888.xyz/2022/05/04/hzyIX.png)

    å‡è®¾Tå¤§äº1ï¼Œå¯ä»¥ä½¿å…¶åˆ†å¸ƒå˜å¾—æ¯”è¾ƒé›†ä¸­ã€å¹³æ»‘ï¼›è¿™å¯¼è‡´çš„å¥½å¤„å¯ä»¥ç»™äºˆstudent neté¢å¤–çš„ä¿¡æ¯ï¼ˆæŸå’ŒæŸé•¿å¾—åƒï¼‰ï¼Œè€Œåˆ†ç±»ç»“æœä¿æŒä¸å˜ã€‚
*   å¦ä¸€ä¸ªå°trickï¼šåœ¨çŸ¥è¯†è’¸é¦ä¸­å¯¹äºå­¦ç”Ÿç½‘ç»œå¯¹äºè€å¸ˆç½‘ç»œçš„æ˜ å°„ä¸­å¯ä»¥å¤šåšä¸€äº›é™åˆ¶ï¼Œä¸€èˆ¬ç»“æœä¼šå¥½ä¸€äº›ã€‚è­¬å¦‚è¯´ï¼šå¯¹äº12å±‚çš„è€å¸ˆç½‘ç»œï¼Œæœ‰äººå¯¹æ¯ä¸€å±‚çš„è¾“å‡ºå¯¹äºå¯¹åº”çš„å­¦ç”Ÿç½‘ç»œåšä¸€æ¬¡å­¦ä¹ .

    å¦‚æœå­¦ç”Ÿç½‘ç»œå’Œè€å¸ˆç½‘ç»œå·®å¤ªå¤šäº†çš„è¯ï¼Œå¯ä»¥æ’å…¥ä¸€ä¸ªä»‹äºä¸­ä»‹ä½œç”¨çš„networkï¼Œè®©å­¦ç”Ÿå»å­¦è¿™ä¸ªnetworkã€‚

    Temperature Tä¹Ÿæ˜¯ä¸€ä¸ªè¶…å‚æ•°ã€‚

    Ensembleäº¦å¯ä»¥å¯¹å‚æ•°åšå¹³å‡ï¼ˆä¸Šé¢è€å¸ˆæ˜¯è¯´å¯¹è¾“å‡ºåšå¹³å‡ï¼‰

## Parameter Quantizationï¼šé‡åŒ–å‚æ•°

*   ä½¿ç”¨æ¯”è¾ƒå°‘çš„ç©ºé—´æ¥å‚¨å­˜ä¸€ä¸ªå‚æ•°ï¼ˆUsing less bits to represent a valueï¼‰

    è­¬å¦‚è¯´ï¼šæŸå¤±éƒ¨åˆ†ç²¾åº¦çš„æƒ…å†µä¸‹ï¼Œå°†16bitçš„å‚æ•°å‹ç¼©åˆ°8bitï¼Œç”šè‡³æ›´å°‘ã€‚è€Œperformanceä¸ä¼šæ‰å¾ˆå¤š
*   å‚æ•°çš„å‹ç¼©ï¼šWeight clustering

    ![image-20220418124525980](https://s1.328888.xyz/2022/05/04/hzGjZ.png)

    å¯¹å‚æ•°ç»„æˆçš„ç½‘ç»œåšä¸€ä¸ªèšç±»ï¼ˆclusteringï¼‰ï¼Œæ•°å€¼æ¥è¿‘çš„æ”¾åœ¨ä¸€èµ·ï¼›äº‹å…ˆè®¾å®šå¥½èšç±»æ•°é‡ï¼ˆå¦‚ä¸Šå›¾æ˜¯4ç±»ï¼‰ï¼›å¯¹äºèšç±»çš„ç»“æœå¾€å¾€ä»¥ç±»å†…å‚æ•°å–å¹³å‡çš„æ–¹å¼ã€‚æœ€åï¼Œåªéœ€è¦è®°å½•ä¸¤æ ·ä¸œè¥¿ï¼šä¸€ä¸ªæ˜¯å­˜å‚¨èšç±»ç»“æœçš„è¡¨ï¼ˆç»´åº¦ç­‰äºèšç±»æ•°é‡ï¼‰ï¼Œå¦ä¸€ä¸ªæ˜¯è®°å½•æ¯ä¸ªå‚æ•°è¢«åˆ†åˆ°å“ªä¸ªç±»ã€‚

    ä¸ºäº†ä¾¿äºè®­ç»ƒï¼Œä¼šè¦æ±‚networkä¹‹é—´çš„å‚æ•°æ¯”è¾ƒæ¥è¿‘ï¼Œä»è€Œå¯ä»¥æ–¹ä¾¿èšç±»ã€‚
*   Represent frequent clusters by less bitsï¼Œrepresent rare clusters by more bitsï¼ŒåŠæ¯”è¾ƒå¸¸å‡ºç°çš„ç”¨æ¯”è¾ƒå°‘bitæ¥ç¼–ç ï¼Œæ¯”è¾ƒç½•è§çš„ç”¨è¾ƒå¤šçš„bitæ¥ç¼–ç ã€‚

    ä¸¾ä¾‹ï¼š_éœå¤«æ›¼ç¼–ç ï¼ˆHuffman encodinigï¼‰_

### Binary Weights

æ¯”è¾ƒæç«¯çš„æƒ…å†µï¼šåªç”¨ä¸€ä¸ªbitæ¥å­˜å‚¨å‚æ•°ã€‚ä½ çš„å‚æ•°è¦ä¸æ˜¯+1è¦ä¸å°±æ˜¯-1ã€‚

> å‚è€ƒæ–‡çŒ®ï¼š[Binary Connect](https://arxiv.org/abs/1511.00363)ã€[Binary Network](https://arxiv.org/abs/1602.02830)ã€[XNOR-net](https://arxiv.org/abs/1603.05279)
>
> ä»¥binary connectä¸ºä¾‹ï¼š
>
> ![image-20220418125640627](https://s1.328888.xyz/2022/05/04/hz9JC.png)
>
> ç»“æœä¹Ÿä¸é”™ï¼š
>
> ![image-20220418125735667](https://s1.328888.xyz/2022/05/04/hzJzg.png)
>
> ç¬¬ä¸€è¡Œæ˜¯ä¸€èˆ¬çš„ç½‘ç»œï¼Œç¬¬äºŒä¸‰è¡Œç”¨binary connectç»“æœæ›´å¥½äº†ã€‚binary networkå¯¹ç½‘ç»œåšäº†ä¸€äº›æ›´å¤šçš„é™åˆ¶ï¼Œä»è€Œå¾ˆå¥½çš„å‡å¼±äº†è¿‡æ‹Ÿåˆã€‚

## Architecture Designï¼šç½‘ç»œçš„æ¶æ„è®¾è®¡

> Reviewï¼šstandard CNN
>
> æ¯ä¸ªlayerçš„inputéƒ½æ˜¯ä¸€ä¸ªfeature mapï¼ˆä»¥ä¸‹å›¾ä¸ºä¾‹ï¼šæœ‰2ä¸ªchannelï¼‰ã€‚è¾“å…¥feature mapçš„channelçš„æ•°é‡ç­‰äºfilterï¼ˆç«‹æ–¹ä½“å½¢çŠ¶ï¼‰çš„é«˜åº¦ã€‚ç”¨è¿™ä¸ªfilteræ‰«è¿‡feature mapä¼šå¾—åˆ°ä¸€ä¸ªè¾“å‡ºfeature mapï¼Œæœ‰å‡ ä¸ªfilteré‚£ä¹ˆoutputçš„feture mapå°±æœ‰å‡ ä¸ªchannelã€‚ï¼ˆæ€»å…±å‚æ•°é‡æ˜¯72ä¸ªï¼‰
>
> ![image-20220418131048612](https://s1.328888.xyz/2022/05/04/hzLn1.png)

### Depthwise Separable Convolution

#### æ“ä½œ

*   æ­¥éª¤ä¸€ï¼š**Depthwise Convolution**ï¼ˆå’ŒCNNä¸åŒï¼šè¯¥æ–¹æ³•ä¸‹è¾“å…¥å’Œè¾“å‡ºchannelæ•°é‡æ˜¯ä¸€æ ·çš„ï¼‰

    ![image-20220418173808908](https://s1.328888.xyz/2022/05/04/hzfet.png)

    Filter number = Input channel number and Each filter only considers one channel.ï¼ˆæœ‰å‡ ä¸ªchannelå°±æœ‰å‡ ä¸ªfilterï¼Œæ¯ä¸ªfilteråªè´Ÿè´£å…¶å¯¹åº”çš„channelã€‚ï¼‰

    filteræ˜¯ä¸€ä¸ª$k \times k$çš„çŸ©é˜µï¼Œæ¯ä¸ªfilteråªåœ¨ä¸€ä¸ªchannelä¸Šæ»‘æ¥æ»‘å»ï¼ˆåšconvolutionï¼‰ï¼Œå¾—åˆ°å¯¹åº”çš„feature mapã€‚

    channelå’Œchannelä¹‹é—´æ²¡æœ‰ä»»ä½•äº’åŠ¨ã€‚ï¼ˆå±€é™æ€§ï¼‰Depthwise Convolutionå¯¹äºè·¨channelçš„patternæ˜¯æ— èƒ½ä¸ºåŠ›çš„ã€‚äºæ˜¯ğŸ‘‡
*   æ­¥éª¤äºŒï¼š**Pointwise Convolution**

    ![image-20220418174222701](https://s1.328888.xyz/2022/05/04/hziDe.png)

    ç±»ä¼¼äºä¸€èˆ¬çš„å·ç§¯å±‚ï¼Œå¯¹äºå«æœ‰2 channelçš„è¾“å…¥ï¼Œæœ‰ä¸æ­¢ä¸€ä¸ªçš„fiiterï¼Œä½†æ˜¯æ¯ä¸€ä¸ªfilterçš„kernel sizeé™åˆ¶ä¸º$1 \times 1$ã€‚æ¯ä¸ªfilteråˆ†åˆ«æ‰«è¿‡è¾“å…¥åšconvolutionè¾“å‡ºå¾—åˆ°å¯¹åº”çš„feature mapã€‚

ä»¥ä¸Šä¸¤æ­¥éª¤ï¼š æ­¥éª¤ä¸€è€ƒè™‘åˆ°channelå†…çš„å…³ç³»ï¼›æ­¥éª¤äºŒè€ƒè™‘åˆ°channelé—´çš„å…³ç³»ã€‚åˆ†åˆ«éœ€è¦$3 \times 3 \times 2 = 18$ä¸ªå‚æ•°ä»¥åŠ$2 \times 4 = 8$ä¸ªå‚æ•°ï¼Œæ€»å…±éœ€è¦26ä¸ªå‚æ•°ï¼ˆç›¸æ¯”ä¸€èˆ¬çš„å·ç§¯å±‚å°‘äº†å¾ˆå¤šï¼‰ã€‚

#### åŒä¸€èˆ¬çš„å·ç§¯å±‚æ¯”è¾ƒå‚æ•°é‡

$I:$è¾“å…¥feature mapçš„channelæ•°é‡

$O:$è¾“å‡ºfeature mapçš„channelæ•°é‡

$k \times k:$filterå°ºå¯¸

![image-20220418174932251](https://s1.328888.xyz/2022/05/04/hzrwO.png)

ç®€å•è®¡ç®—

$$
\frac{k \times k \times I + I \times O}{k \times k \times I \times O} \\ = \frac{1}{O} + \frac{1}{k \times k} < 1
$$

#### åŸç†

**Low rank approximationâ€”â€”æ¥å‡å°‘ä¸€å±‚networkçš„å‚æ•°é‡**

åœ¨Depthwise Convolutionå’ŒPointwise Convolutionå‡ºç°ä¹‹å‰å‡å°‘networkå‚æ•°çš„æ–¹æ³•ã€‚

![image-20220418175515420](https://s1.328888.xyz/2022/05/04/hz2Bq.png)

è§£é‡Šä¸Šå›¾ï¼šå¯¹äºåŸæ¥çš„ä¸€å±‚ç½‘ç»œ$W$ï¼Œåˆ†åˆ«æœ‰Nä¸ªç¥ç»å…ƒï¼Œæœ‰Mä¸ªç¥ç»å…ƒï¼Œé‚£ä¹ˆæ€»å…±å‚æ•°é‡ä¸º$M \times N$ï¼›Low rank approximationæ–¹æ³•è®¾è®¡ä¸€ä¸ªkä¸ªç¥ç»å…ƒçš„linearå±‚æ”¾åœ¨ç½‘ç»œ$W$ä¹‹é—´ï¼ˆæ‹†æˆä¸¤å±‚$U,V$ï¼‰ï¼Œæ­¤æ—¶å‚æ•°é‡ä¸º$M \times k + k \times N$ã€‚

å¦‚æœ$k << M,N$ï¼Œé‚£ä¹ˆ$M \times N>> k \times (M + N)$ ,å‚æ•°é‡å¤§å¤§å‡å°‘äº†ã€‚å±€é™æ€§åœ¨äºå¯¹äºåŸç½‘ç»œ$W$æœ‰ä¸€å®šé™åˆ¶ï¼Œ$W$çš„ç§©è¦å°äº$k$ï¼Œè¿™æ—¶æ‰èƒ½æ‹†æˆä¸¤ä¸ªç½‘ç»œï¼ˆçŸ©é˜µï¼‰ã€‚

**Depthwise Separable Convolutionçš„åŸç†**

![image-20220418180804723](https://s1.328888.xyz/2022/05/04/hzA2P.png)

åŸæ¥çš„å·ç§¯å±‚ï¼Œ18ä¸ªæ•°å€¼ç›´æ¥ç»è¿‡å·ç§¯å±‚å¾—åˆ°ä¸€ä¸ªæ•°å€¼ï¼ˆç»“æœï¼‰ï¼›è€ŒDepthwise Convolutionåˆ™æ˜¯æŠŠè¿™ä¸ªè¿‡ç¨‹æ‹†æˆä¸¤ä¸ªé˜¶æ®µï¼Œæˆ–è€…è¯´æŠŠä¸€ä¸ªç½‘ç»œæ‹†æˆä¸¤ä¸ªç½‘ç»œã€‚

### More Architecture Design

* [SqueezeNet](https://arxiv.org/abs/1602.07360)
* [MobileNet](https://arxiv.org/abs/1704.04861) ï¼Œæˆ‘å¥½åƒç»å¸¸çœ‹åˆ°è¿™ä¸ªâ€¦
* [ShuffleNet](https://arxiv.org/abs/1707.01083)
* [Xception](https://arxiv.org/abs/1610.02357)
* [GhostNet](https://arxiv.org/abs/1911.11907)

## Dynamic Computationï¼šåŠ¨æ€è®¡ç®—

> ç›¸æ¯”å‰é¢å››ä¸ªæŠ€æœ¯ä¸åŒçš„ç›®æ ‡ï¼šå¸Œæœ›networkè‡ªç”±åœ°è°ƒæ•´å®ƒçš„è¿ç®—é‡

æœ‰æ—¶å€™ï¼ŒåŒæ ·çš„æ¨¡å‹å¯èƒ½ä¼šè·‘åœ¨ä¸åŒçš„devicesä¸Šé¢ï¼Œdeviceä¹‹é—´æœ‰ç€å°ºå¯¸å’Œè¿ç®—èµ„æºä¸Šçš„å·®å¼‚ï¼›å¯¹äºåŒä¸€ä¸ªdeviceï¼Œä¹Ÿéœ€è¦æ ¹æ®è®¾å¤‡çŠ¶æ€æ¥è°ƒæ•´è¿ç®—é‡çš„å¤§å°ï¼ˆè­¬å¦‚æ‰‹æœºç”µé‡ï¼‰ã€‚æ‰€ä»¥æˆ‘ä»¬éœ€è¦networkå¯ä»¥åšåˆ°é€‚åº”è®¾å¤‡æƒ…å†µæ¥è‡ªåŠ¨è°ƒæ•´è¿ç®—é‡ã€‚

![image-20220418191759773](https://s1.328888.xyz/2022/05/04/hzhIm.png)

å¦‚æœæˆ‘ä»¬é€šè¿‡è®­ç»ƒä¸€å¤§å †çš„networkï¼Œæˆ–è®¸èƒ½è§£å†³è¿ç®—èµ„æºé€‚åº”çš„é—®é¢˜ï¼Œä½†æ˜¯å…¶ä¼šå ç”¨å¤§é‡çš„å­˜å‚¨ç©ºé—´ã€‚

### Dynamic Depth

ä¸€ä¸ªå¯èƒ½çš„æ–¹å‘æ˜¯ï¼Œé€šè¿‡è®©networkè‡ªç”±çš„è°ƒæ•´å®ƒçš„æ·±åº¦ï¼Œä»è€Œè®©å…¶è°ƒæ•´è¿ç®—èµ„æºçš„éœ€æ±‚ã€‚

ä»¥åšå›¾åƒåˆ†ç±»ä¸ºä¾‹ï¼šè®­ç»ƒä¸€ä¸ªå¾ˆæ·±çš„network

![image-20220418192221629](https://s1.328888.xyz/2022/05/04/hz4vA.png)

åœ¨layerå’Œlayerä¹‹é—´å†åŠ ä¸Šä¸€ä¸ªextra layerï¼Œå…¶ä½œç”¨ï¼šæ ¹æ®å‰ä¸€ä¸ªéšè—å±‚ï¼ˆhidden layerï¼‰çš„è¾“å‡ºæ¥å†³å®šç°åœ¨åˆ†ç±»çš„ç»“æœåº”è¯¥æ˜¯æ€æ ·çš„ã€‚

å½“è¿ç®—èµ„æºéå¸¸å……è¶³çš„æ—¶å€™ï¼Œå¯ä»¥è·‘é€šæ•´ä¸ªnetworkï¼Œå¾—åˆ°æœ€ç»ˆçš„åˆ†ç±»ç»“æœï¼›è€Œå¦‚æœè¿ç®—èµ„æºæ¯”è¾ƒå±€é™ï¼Œé‚£ä¹ˆå†³å®šå†networkçš„å“ªä¸€ä¸ªextra layerè‡ªè¡Œè¾“å‡ºï¼Œä½œä¸ºç»“æœã€‚

è®­ç»ƒçš„è¿‡ç¨‹ï¼šè®©ground truthï¼ˆ$\hat{y}$ï¼‰è·Ÿæ¯ä¸€å±‚extra layerä»¥åŠæœ€åçš„outputçš„è·ç¦»$e\_i$ç»Ÿç»ŸåŠ èµ·æ¥ï¼Œå¾—åˆ°lossçš„è®¡ç®—å…¬å¼ï¼š$L = e\_1 + e\_2 + ... + e\_L$ï¼Œç„¶åminimizeè¿™ä¸ªlosså‡½æ•°ã€‚ç›®å‰æ¥çœ‹ï¼Œè¿™å¹¶ä¸æ˜¯æœ€å¥½çš„æ–¹æ³•ã€‚ç›®å‰æ¯”è¾ƒä¼˜å¼‚çš„è®­ç»ƒæ–¹æ³•å‚è€ƒ[Multi-Scale Dense Network (MSDNet)](https://arxiv.org/abs/1703.09844)ã€‚

### Dynamic Width

> è®©networkè‡ªç”±å†³å®šå®ƒçš„å®½åº¦

åŒä¸€å¼ å›¾ç‰‡ä¸¢è¿›ä¸åŒå®½åº¦çš„networkï¼Œæ¯ä¸€ä¸ªnetworkä¼šæœ‰ä¸åŒçš„è¾“å‡ºï¼Œç›®æ ‡æ˜¯æ‰€æœ‰çš„è¾“å‡ºéƒ½å’Œground truthè¶Šæ¥è¿‘è¶Šå¥½ã€‚losså‡½æ•°ï¼š$L = e\_1 + e\_2 + e\_3$

![image-20220418193357209](https://s1.328888.xyz/2022/05/04/hzqJS.png)

ä»¥ä¸Šå›¾ï¼šåŒä¸€ä¸ªnetworkä½†æ˜¯å®½åº¦ä¸åŒï¼ˆneuronçš„ä½¿ç”¨ç‡100%ã€75%ã€50%ï¼‰ã€‚

ä»¥ä¸Šè‡³å°‘ç®€ç•¥çš„æƒ³æ³•ï¼Œå®è·µè¿™æ ·ç®€å•çš„åšæ³•æ˜¯æœ‰é—®é¢˜çš„ã€‚å…³äºdynamic widthå¯ä»¥å‚è€ƒ[Slimmable Neural Networks](https://arxiv.org/abs/1812.08928)ã€‚

### Computation based on Sample Difficultyï¼šè®©networkæ ¹æ®æƒ…å¢ƒè‡ªè¡Œå†³å®šè‡ªèº«çš„æ·±åº¦æˆ–å®½åº¦

ä¸åŒå›¾åƒå­˜åœ¨ä¸åŒçš„è¯†åˆ«éš¾åº¦ğŸ‘‡

![image-20220418193844859](https://s1.328888.xyz/2022/05/04/hzDzR.png)

ç®€å•çš„å›¾ç‰‡ï¼Œç¬¬ä¸€å±‚å°±åœä¸‹æ¥ï¼›å›°éš¾çš„å›¾ç‰‡å¯èƒ½éœ€è¦è·‘å¥½å¤šå±‚ã€‚

è¿™ä¸ªé—®é¢˜ç›¸å…³çš„æ–‡ç« ï¼š

* [SkipNet: Learning Dynamic Routing in Convolutional Networks](https://arxiv.org/abs/1711.09485)
* [Runtime Neural Pruning](https://paperswithcode.com/paper/runtime-neural-pruning)
* [BlockDrop: Dynamic Inference Paths in Residual Networks](https://arxiv.org/abs/1711.08393)

## Concluding Remarks

å‰å››ä¸ªæŠ€æœ¯ä¸æ˜¯äº’æ–¥çš„ï¼Œå¯ä»¥åœ¨networkä¸­ä¸€èµ·è¢«ä½¿ç”¨ã€‚
